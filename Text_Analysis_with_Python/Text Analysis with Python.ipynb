{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <h1>Digital Tools and Methods for the Humanities and Social Sciences</h1>\n",
    "  <img src=\"https://raw.githubusercontent.com/sul-cidr/Workshops/master/cidr-logo.no-text.240x140.png\" alt=\"Center for Interdisciplinary Digital Research @ Stanford\"/>\n",
    "</center>\n",
    "\n",
    "<h1>Text Analysis with Python</h1>\n",
    "\n",
    "## Front-Matter\n",
    "\n",
    "### Instructors\n",
    "- Scott Bailey (CIDR), <em>scottbailey@stanford.edu</em>\n",
    "- Simon Wiles (CIDR), <em>simon.wiles@stanford.edu</em>\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "Develop practical knowledge of an end-to-end workflow for text analysis in Python using two specific libraries: spaCy and textacy.\n",
    "\n",
    "- Import data\n",
    "- Clean/preprocess text data\n",
    "- Analyze single documents\n",
    "- Analyze a full corpus\n",
    "\n",
    "\n",
    "### Topics\n",
    "\n",
    "<mark>TODO!!</mark>\n",
    "\n",
    "\n",
    "### Jupyter Notebooks and Google Colaboratory\n",
    "\n",
    "Jupyter notebooks are a way to write and run Python code in an interactive way. They're quickly becoming a standard way of putting together data, code, and written explanations or visualizations into a single document and sharing that. There are a lot of ways that you can run Jupyter notebooks, including just locally on your computer, but we've decided to use Google's Colaboratory notebook platform for this workshop.  Colaboratory is “a Google research project created to help disseminate machine learning education and research.”  If you would like to know more about Colaboratory in general, you can visit the [Welcome Notebook](https://colab.research.google.com/notebooks/welcome.ipynb).\n",
    "\n",
    "Using the Google Colaboratory platform allows us to focus on learning and writing Python in the workshop rather than on setting up Python, which can sometimes take a bit of extra work depending on platforms, operating systems, and other installed applications. If you'd like to install a Python distribution locally, though, we have some instructions (with gifs!) on installing Python through the Anaconda distribution, which will also help you handle virtual environments: https://github.com/sul-cidr/Workshops/wiki/Installing-and-Configuring-Anaconda-and-Jupyter-Notebooks\n",
    "\n",
    "If you run into problems, or would like to look into other ways of installing Python or handling virtual environments, feel free to send us an email (contact-cidr@stanford.edu) or visit us during our [consulting hours](https://library.stanford.edu/research/cidr/consulting).\n",
    "\n",
    "### Environment\n",
    "If you would prefer to use Anaconda or your own local installation of python or Jupyter Notebooks, for this workshop you will need an environment with the following packages installed and available:\n",
    "- `spacy`\n",
    "- `textacy`\n",
    "\n",
    "<mark>update these ↑</mark>\n",
    "\n",
    "Please note that we will not have time during the workshop to support you with problems related to a local environment, and we do recommend using the Colaboratory notebooks if you are at all unsure.\n",
    "\n",
    "### Evaluation survey\n",
    "At the end of the workshop, we would be very grateful if you can, please, spend 1 minute answering a few questions that will help us to continue our workshop series.\n",
    "- <mark>link here</mark>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with a single document with spaCy\n",
    "\n",
    "Let's start by learning how spaCy works, and using it to start analyzing a single textual document. We'll work with some sample data throughout, but talk through importing larger corpora later in the workshop. \n",
    "\n",
    "For now, we'll start with imports, setting up the model, and working with a short text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy uses pre-trained neural network models to process text. Here we're going to download and use a medium-sized English multi-task CNN, which has high accuracy for part of speech tagging, entity recognition, and includes word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you were working locally, you would follow the instruction on spaCy's site\n",
    "# about installing models and loading them. We're taking advantage of the fact\n",
    "# that Google Colab pre-installs Spacy and a number of models for us.\n",
    "import en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the language model\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From H.G. Well's A Short History of the World, Project Gutenberg \n",
    "text = \"\"\"Even under the Assyrian monarchs and especially under\n",
    "Sardanapalus, Babylon had been a scene of great intellectual\n",
    "activity.  {111} Sardanapalus, though an Assyrian, had been quite\n",
    "Babylon-ized.  He made a library, a library not of paper but of\n",
    "the clay tablets that were used for writing in Mesopotamia since\n",
    "early Sumerian days.  His collection has been unearthed and is\n",
    "perhaps the most precious store of historical material in the\n",
    "world.  The last of the Chaldean line of Babylonian monarchs,\n",
    "Nabonidus, had even keener literary tastes.  He patronized\n",
    "antiquarian researches, and when a date was worked out by his\n",
    "investigators for the accession of Sargon I he commemorated the\n",
    "fact by inscriptions.  But there were many signs of disunion in\n",
    "his empire, and he sought to centralize it by bringing a number of\n",
    "the various local gods to Babylon and setting up temples to them\n",
    "there.  This device was to be practised quite successfully by the\n",
    "Romans in later times, but in Babylon it roused the jealousy of\n",
    "the powerful priesthood of Bel Marduk, the dominant god of the\n",
    "Babylonians.  They cast about for a possible alternative to\n",
    "Nabonidus and found it in Cyrus the Persian, the ruler of the\n",
    "adjacent Median Empire.  Cyrus had already distinguished himself\n",
    "by conquering Croesus, the rich king of Lydia in Eastern Asia\n",
    "Minor.  {112} He came up against Babylon, there was a battle\n",
    "outside the walls, and the gates of the city were opened to him\n",
    "(538 B.C.).  His soldiers entered the city without fighting.  The\n",
    "crown prince Belshazzar, the son of Nabonidus, was feasting, the\n",
    "Bible relates, when a hand appeared and wrote in letters of fire\n",
    "upon the wall these mystical words: _\"Mene, Mene, Tekel,\n",
    "Upharsin,\"_ which was interpreted by the prophet Daniel, whom he\n",
    "summoned to read the riddle, as \"God has numbered thy kingdom and\n",
    "finished it; thou art weighed in the balance and found wanting and\n",
    "thy kingdom is given to the Medes and Persians.\"  Possibly the\n",
    "priests of Bel Marduk knew something about that writing on the\n",
    "wall.  Belshazzar was killed that night, says the Bible.\n",
    "Nabonidus was taken prisoner, and the occupation of the city was\n",
    "so peaceful that the services of Bel Marduk continued without\n",
    "intermission.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we pass the text into the NLP model, spaCy processes the entire text and makes many features available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "The doc created by spaCy immediately provides access to the word level tokens of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:15]:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these tokens has a number of properties, and we'll look a bit more closely at this in a minute when we think about preprocessing texts, but let's continue our quick tour. \n",
    "\n",
    "spaCy also automatically provides sentence level tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent.text + \"\\n--\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can collect both words and sentences into standard Python data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sent.text for sent in doc.sents]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.text for token in doc]\n",
    "words[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can we do with tokens like this?\n",
    "\n",
    "Let's start with cleaning the text and counting to see what we can learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the common things we do in text analysis is to remove punctuation\n",
    "\n",
    "no_punct = [token for token in doc if token.is_punct == False]\n",
    "for token in no_punct[:20]:\n",
    "  print(token.text, token.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has worked, but left in new line characters and spaces\n",
    "no_punct_or_space = [token for token in doc if token.is_punct == False and token.is_space == False]\n",
    "for token in no_punct_or_space[:30]:\n",
    "  print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say we also want to remove numbers, and lowercase everything\n",
    "lower_alpha = [token.lower_ for token in no_punct_or_space if token.is_alpha == True]\n",
    "lower_alpha[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other common bit of preprocessing is to remove stopwords, that is, the common words in a language that don't convey the information that we are looking for in our analysis. For example, if we looked for the most common words in a text, we would want to remove stopwords so that we don't only get words such as 'a,' 'the,' and 'and.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = [token.lower_ for token in no_punct_or_space if token.is_alpha == True and token.is_stop == False]\n",
    "clean[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a list of lower-cased tokens that doesn't contain punctuation, white-space, numbers, or stopwords. Depending on our analysis, we may or may not want to do this much cleaning. But, it is good to understand how much we can do just with spaCy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then look at what we can do now that we have groups of tokens at different lengths. We can start with just counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of tokens in document: \", len(doc))\n",
    "print(\"Number of tokens in cleaned document: \", len(clean))\n",
    "print(\"Number of unique tokens in cleaned document: \", len(set(clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "full_counter = Counter([token.lower_ for token in doc])\n",
    "full_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_counter = Counter([token.lower_ for token in doc])\n",
    "full_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_counter = Counter(clean)\n",
    "cleaned_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why do we have to use a list comprehension for the non-clean doc while we can just pass a variable directly for the cleaned set of tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn to the other aspects of the text that spaCy exposes for us. Depending on what questions we might have about the text, these will be more or less helpful. \n",
    "\n",
    "We'll start with parts of speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy provides two levels of POS tagging. Here's the more general.\n",
    "for token in doc[:30]:\n",
    "  print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also have the more specific Penn Treenbank tags.\n",
    "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "for token in doc[:30]:\n",
    "  print(token.text, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can accumulate the groups of tokens by way of these in order understand distributions of parts of speech throughout the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [token for token in doc if token.pos_ == \"NOUN\"]\n",
    "verbs = [token for token in doc if token.pos_ == \"VERB\"]\n",
    "proper_nouns = [token for token in doc if token.pos_ == \"PROPN\"]\n",
    "adjectives = [token for token in doc if token.pos_ == \"ADJ\"]\n",
    "adverbs = [token for token in doc if token.pos_ == \"ADV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts = {\n",
    "    \"nouns\": len(nouns),\n",
    "    \"verbs\": len(verbs),\n",
    "    \"proper_nouns\": len(proper_nouns),\n",
    "    \"adjectives\": len(adjectives),\n",
    "    \"adverbs\": len(adverbs) \n",
    "}\n",
    "\n",
    "pos_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy also provides full dependency parsing, but we're going to leave that alone for the moment. We'll turn instead to named entity recognition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "  print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we only care about geo-political entities or locations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_filtered = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in [\"GPE\", \"LOC\"]]\n",
    "ent_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy also has a nice built-in visualizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's shift to thinking about a whole corpus rather than a single document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In doing so, we could keep working with spaCy directly if the features that it exposes help us answer the research questions we are asking. \n",
    "\n",
    "Instead, though, we're going to take advantage of textacy, a library built on spaCy that adds features, including a sense of a Corpus and built in analytics on it. \n",
    "\n",
    "We'll use some of the data that is included in textacy as our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "import textacy.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll work with some Supreme Court cases: https://chartbeat-labs.github.io/textacy/_modules/textacy/datasets/supreme_court.html\n",
    "data = textacy.datasets.SupremeCourt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have here is a collection of Supreme Court decisions, both full text and metadata. \n",
    "\n",
    "Let's look at a single one to see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single = list(data.texts(limit=1))[0]\n",
    "single[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = data.records(limit=20)\n",
    "next(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textacy includes the idea of a corpus, while spaCy only has an idea of a single documents, though you can compose documents in standard Python data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = textacy.Corpus(nlp, data=records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0]._.preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the type of `corpus[0]` that it is a Doc - this is effectively a spaCy doc with all of the calculated features. Textacy does give you some capacity to work with those features through it's API, and also exposes new features, such as ngrams and ranking algorithms for single documents. We'll come back to then once we work a bit at the corpus level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter this course based on metadata once we make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll find all the cases where the number of justices voting in the majority was greater than 6. \n",
    "recent = list(corpus.get(lambda doc: doc._.meta[\"n_maj_votes\"] > 6))\n",
    "len(recent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what we get out of the box from textacy once we've built a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of documents: \", corpus.n_docs)\n",
    "print(\"number of sentences: \", corpus.n_sents)\n",
    "print(\"number of tokens: \", corpus.n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll pass as_strings so that the results we look at will give us strings rather than unique ids.\n",
    "counts = corpus.word_counts(as_strings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(counts.items(), key=lambda x: x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_doc_counts = corpus.word_doc_counts(weighting=\"count\", smooth_idf=True, filter_stops=True, as_strings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(word_doc_counts.items(), key=lambda x:x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should note that these are not tf-idf values, which are term frequencies for individual docs weighted by the inverse document frequency. This is a measure of the number of docs the words appear in weighted by inverse document frequency. We're still getting a sense of which words across the corpus and in the context of the corpus seem to have the most importance, if document frequency is a proxy for importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue with corpus level analysis by taking advantage of textacy's vectorizer class, which wraps functionality from scikit-learn. We could just work directly in scikit-learn, but it can be nice for mental overhead to learn one library and be able to do a great deal with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.vsm\n",
    "\n",
    "# We'll create a vectorizer, sticking with the normal term frequency defaults but\n",
    "# discarding words that appear in less than 3 documents or more than 95% of documents. \n",
    "#We could vectorizer according to tf-idf as well.\n",
    "vectorizer = textacy.vsm.Vectorizer(min_df=3, max_df=.95, max_n_terms=200)\n",
    "tokenized_corpus = (doc._.to_terms_list(ngrams=1, as_strings=True,\n",
    "                                        filter_punct=True, \n",
    "                                        filter_stops=True, \n",
    "                                        filter_nums=True \n",
    "                                        ) for doc in corpus)\n",
    "dtm = vectorizer.fit_transform(tokenized_corpus)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now have a matrix representation of our corpus, where rows are documents, and columns (or features) are words from the corpus. The value at any given point is the number of times that the word appears in that document. Once we have a document-term matrix, we could do a few different things with it, just within textacy, though we could take it and pass it into different algorithms within scikit-learn or other libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first look at some of the terms\n",
    "vectorizer.terms_list[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we are still getting a number of terms which ought to be filtered out, such as numbers and punctuation. We would want to clean this up more before vectorizing in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to do:\n",
    "\n",
    "- textacy: extract ngrams\n",
    "- textacy: pagerank and sgrank\n",
    "- word co-occurence matrices?\n",
    "- custom cleaning?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "206px",
    "width": "555px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
