{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "206px",
        "width": "555px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "Text Analysis with Python.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGRR3FVUVyQb",
        "colab_type": "text"
      },
      "source": [
        "<center>\n",
        "  <h1>Digital Tools and Methods for the Humanities and Social Sciences</h1>\n",
        "  <img src=\"https://raw.githubusercontent.com/sul-cidr/Workshops/master/cidr-logo.no-text.240x140.png\" alt=\"Center for Interdisciplinary Digital Research @ Stanford\"/>\n",
        "</center>\n",
        "\n",
        "<h1>Text Analysis with Python</h1>\n",
        "\n",
        "## Front-Matter\n",
        "\n",
        "### Instructors\n",
        "- Scott Bailey (CIDR), <em>scottbailey@stanford.edu</em>\n",
        "- Simon Wiles (CIDR), <em>simon.wiles@stanford.edu</em>\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "Develop practical knowledge of an end-to-end workflow for text analysis in Python using two specific libraries: spaCy and textacy.\n",
        "\n",
        "- Import data\n",
        "- Clean/preprocess text data\n",
        "- Analyze single documents\n",
        "- Analyze a full corpus\n",
        "\n",
        "\n",
        "### Topics\n",
        "\n",
        "- Document Tokenization\n",
        "- Part-of-Speech (POS) Tagging\n",
        "- Named-Entity Recognition (NER)\n",
        "- Corpus Analysis and Vectorization\n",
        "\n",
        "\n",
        "### Jupyter Notebooks and Google Colaboratory\n",
        "\n",
        "Jupyter notebooks are a way to write and run Python code in an interactive way. They're quickly becoming a standard way of putting together data, code, and written explanations or visualizations into a single document and sharing that. There are a lot of ways that you can run Jupyter notebooks, including just locally on your computer, but we've decided to use Google's Colaboratory notebook platform for this workshop.  Colaboratory is “a Google research project created to help disseminate machine learning education and research.”  If you would like to know more about Colaboratory in general, you can visit the [Welcome Notebook](https://colab.research.google.com/notebooks/welcome.ipynb).\n",
        "\n",
        "Using the Google Colaboratory platform allows us to focus on learning and writing Python in the workshop rather than on setting up Python, which can sometimes take a bit of extra work depending on platforms, operating systems, and other installed applications. If you'd like to install a Python distribution locally, though, we have some instructions (with gifs!) on installing Python through the Anaconda distribution, which will also help you handle virtual environments: https://github.com/sul-cidr/Workshops/wiki/Installing-and-Configuring-Anaconda-and-Jupyter-Notebooks\n",
        "\n",
        "If you run into problems, or would like to look into other ways of installing Python or handling virtual environments, feel free to send us an email (contact-cidr@stanford.edu) or visit us during our [consulting hours](https://library.stanford.edu/research/cidr/consulting).\n",
        "\n",
        "### Environment\n",
        "If you would prefer to use Anaconda or your own local installation of python or Jupyter Notebooks, for this workshop you will need an environment with the following packages installed and available:\n",
        "- `spacy`\n",
        "- `textacy`\n",
        "\n",
        "Please note that we will not have time during the workshop to support you with problems related to a local environment, and we do recommend using the Colaboratory notebooks if you are at all unsure.\n",
        "\n",
        "### Evaluation survey\n",
        "At the end of the workshop, we would be very grateful if you can, please, spend 1 minute answering a few questions that will help us to continue our workshop series.\n",
        "- <mark>link here</mark>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEFaMNm3VyQf",
        "colab_type": "text"
      },
      "source": [
        "# Document-level Analysis with `spaCy`\n",
        "\n",
        "Let's start by learning how spaCy works, and using it to start analyzing a single textual document. We'll work with some sample data throughout, but talk through importing larger corpora later in the workshop. \n",
        "\n",
        "For now, we'll start with imports, setting up the model, and working with a short text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1u3OoHQVyQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRVkmEOvVyQq",
        "colab_type": "text"
      },
      "source": [
        "spaCy uses pre-trained neural network models to process text. Here we're going to download and use a medium-sized English multi-task CNN, which has high accuracy for part of speech tagging, entity recognition, and includes word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3lrUP1cVyQs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "22c30b0d-b467-43f9-d7d1-2df4daa016fb"
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 1.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126237 sha256=937da0eb8577463473535115a710f6e7a6973a0901927b2f835c95d79ec3c2db\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-40amckw2/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31NWduWIVyQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Once we've installed the model, we can load it like any other Python library\n",
        "import en_core_web_md"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT2rin_fVyQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the language model\n",
        "nlp = en_core_web_md.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQYz476fVyRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From H.G. Well's A Short History of the World, Project Gutenberg \n",
        "text = \"\"\"Even under the Assyrian monarchs and especially under\n",
        "Sardanapalus, Babylon had been a scene of great intellectual\n",
        "activity.  {111} Sardanapalus, though an Assyrian, had been quite\n",
        "Babylon-ized.  He made a library, a library not of paper but of\n",
        "the clay tablets that were used for writing in Mesopotamia since\n",
        "early Sumerian days.  His collection has been unearthed and is\n",
        "perhaps the most precious store of historical material in the\n",
        "world.  The last of the Chaldean line of Babylonian monarchs,\n",
        "Nabonidus, had even keener literary tastes.  He patronized\n",
        "antiquarian researches, and when a date was worked out by his\n",
        "investigators for the accession of Sargon I he commemorated the\n",
        "fact by inscriptions.  But there were many signs of disunion in\n",
        "his empire, and he sought to centralize it by bringing a number of\n",
        "the various local gods to Babylon and setting up temples to them\n",
        "there.  This device was to be practised quite successfully by the\n",
        "Romans in later times, but in Babylon it roused the jealousy of\n",
        "the powerful priesthood of Bel Marduk, the dominant god of the\n",
        "Babylonians.  They cast about for a possible alternative to\n",
        "Nabonidus and found it in Cyrus the Persian, the ruler of the\n",
        "adjacent Median Empire.  Cyrus had already distinguished himself\n",
        "by conquering Croesus, the rich king of Lydia in Eastern Asia\n",
        "Minor.  {112} He came up against Babylon, there was a battle\n",
        "outside the walls, and the gates of the city were opened to him\n",
        "(538 B.C.).  His soldiers entered the city without fighting.  The\n",
        "crown prince Belshazzar, the son of Nabonidus, was feasting, the\n",
        "Bible relates, when a hand appeared and wrote in letters of fire\n",
        "upon the wall these mystical words: _\"Mene, Mene, Tekel,\n",
        "Upharsin,\"_ which was interpreted by the prophet Daniel, whom he\n",
        "summoned to read the riddle, as \"God has numbered thy kingdom and\n",
        "finished it; thou art weighed in the balance and found wanting and\n",
        "thy kingdom is given to the Medes and Persians.\"  Possibly the\n",
        "priests of Bel Marduk knew something about that writing on the\n",
        "wall.  Belshazzar was killed that night, says the Bible.\n",
        "Nabonidus was taken prisoner, and the occupation of the city was\n",
        "so peaceful that the services of Bel Marduk continued without\n",
        "intermission.\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR5v_iE3VyRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9pYgwcqVyRL",
        "colab_type": "text"
      },
      "source": [
        "Once we pass the text into the NLP model, spaCy processes the entire text and makes many features available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnkTWvuwVyRN",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "The doc created by spaCy immediately provides access to the word level tokens of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg6EB7WeVyRR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f3c9da3a-86a6-430b-e634-6a56fc79d157"
      },
      "source": [
        "for token in doc[:15]:\n",
        "  print(token)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Even\n",
            "under\n",
            "the\n",
            "Assyrian\n",
            "monarchs\n",
            "and\n",
            "especially\n",
            "under\n",
            "\n",
            "\n",
            "Sardanapalus\n",
            ",\n",
            "Babylon\n",
            "had\n",
            "been\n",
            "a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh2z0VkgVyRW",
        "colab_type": "text"
      },
      "source": [
        "Each of these tokens has a number of properties, and we'll look a bit more closely at this in a minute when we think about preprocessing texts, but let's continue our quick tour. \n",
        "\n",
        "spaCy also automatically provides sentence level tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Rr6LvXVyRY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6e03ce8a-855f-4af3-b1dc-8a6e41ed7af9"
      },
      "source": [
        "for sent in doc.sents:\n",
        "    print(sent.text + \"\\n--\\n\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Even under the Assyrian monarchs and especially under\n",
            "Sardanapalus, Babylon had been a scene of great intellectual\n",
            "activity.  \n",
            "--\n",
            "\n",
            "{111} Sardanapalus, though an Assyrian, had been quite\n",
            "Babylon-ized.  \n",
            "--\n",
            "\n",
            "He made a library, a library not of paper but of\n",
            "the clay tablets that were used for writing in Mesopotamia since\n",
            "early Sumerian days.  \n",
            "--\n",
            "\n",
            "His collection has been unearthed and is\n",
            "perhaps the most precious store of historical material in the\n",
            "world.  \n",
            "--\n",
            "\n",
            "The last of the Chaldean line of Babylonian monarchs,\n",
            "Nabonidus, had even keener literary tastes.  \n",
            "--\n",
            "\n",
            "He patronized\n",
            "antiquarian researches, and when a date was worked out by his\n",
            "investigators for the accession of Sargon I he commemorated the\n",
            "fact by inscriptions.  \n",
            "--\n",
            "\n",
            "But there were many signs of disunion in\n",
            "his empire, and he sought to centralize it by bringing a number of\n",
            "the various local gods to Babylon and setting up temples to them\n",
            "there.  \n",
            "--\n",
            "\n",
            "This device was to be practised quite successfully by the\n",
            "Romans in later times, but in Babylon it roused the jealousy of\n",
            "the powerful priesthood of Bel Marduk, the dominant god of the\n",
            "Babylonians.  \n",
            "--\n",
            "\n",
            "They cast about for a possible alternative to\n",
            "Nabonidus and found it in Cyrus the Persian, the ruler of the\n",
            "adjacent Median Empire.  \n",
            "--\n",
            "\n",
            "Cyrus had already distinguished himself\n",
            "by conquering Croesus, the rich king of Lydia in Eastern Asia\n",
            "Minor.  \n",
            "--\n",
            "\n",
            "{112} He came up against Babylon, there was a battle\n",
            "outside the walls, and the gates of the city were opened to him\n",
            "(538 B.C.).  \n",
            "--\n",
            "\n",
            "His soldiers entered the city without fighting.  \n",
            "--\n",
            "\n",
            "The\n",
            "crown prince Belshazzar, the son of Nabonidus, was feasting\n",
            "--\n",
            "\n",
            ", the\n",
            "Bible relates, when a hand appeared and wrote in letters of fire\n",
            "upon the wall these mystical words: _\"Mene, Mene, Tekel,\n",
            "Upharsin,\"_ which was interpreted by the prophet Daniel, whom he\n",
            "summoned to read the riddle, as \"God has numbered thy kingdom and\n",
            "finished it; thou art weighed in the balance and found wanting and\n",
            "thy kingdom is given to the Medes and Persians.\n",
            "--\n",
            "\n",
            "\"  \n",
            "--\n",
            "\n",
            "Possibly the\n",
            "priests of Bel Marduk knew something about that writing on the\n",
            "wall.  \n",
            "--\n",
            "\n",
            "Belshazzar was killed that night, says the Bible.\n",
            "\n",
            "--\n",
            "\n",
            "Nabonidus was taken prisoner, and the occupation of the city was\n",
            "so peaceful that the services of Bel Marduk continued without\n",
            "intermission.\n",
            "--\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcwG9tH9VyRd",
        "colab_type": "text"
      },
      "source": [
        "We can collect both words and sentences into standard Python data structures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwnDSjL7VyRe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "7cb99dfd-a5b2-478b-b82a-85d5a4ee7f03"
      },
      "source": [
        "sentences = [sent.text for sent in doc.sents]\n",
        "sentences"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Even under the Assyrian monarchs and especially under\\nSardanapalus, Babylon had been a scene of great intellectual\\nactivity.  ',\n",
              " '{111} Sardanapalus, though an Assyrian, had been quite\\nBabylon-ized.  ',\n",
              " 'He made a library, a library not of paper but of\\nthe clay tablets that were used for writing in Mesopotamia since\\nearly Sumerian days.  ',\n",
              " 'His collection has been unearthed and is\\nperhaps the most precious store of historical material in the\\nworld.  ',\n",
              " 'The last of the Chaldean line of Babylonian monarchs,\\nNabonidus, had even keener literary tastes.  ',\n",
              " 'He patronized\\nantiquarian researches, and when a date was worked out by his\\ninvestigators for the accession of Sargon I he commemorated the\\nfact by inscriptions.  ',\n",
              " 'But there were many signs of disunion in\\nhis empire, and he sought to centralize it by bringing a number of\\nthe various local gods to Babylon and setting up temples to them\\nthere.  ',\n",
              " 'This device was to be practised quite successfully by the\\nRomans in later times, but in Babylon it roused the jealousy of\\nthe powerful priesthood of Bel Marduk, the dominant god of the\\nBabylonians.  ',\n",
              " 'They cast about for a possible alternative to\\nNabonidus and found it in Cyrus the Persian, the ruler of the\\nadjacent Median Empire.  ',\n",
              " 'Cyrus had already distinguished himself\\nby conquering Croesus, the rich king of Lydia in Eastern Asia\\nMinor.  ',\n",
              " '{112} He came up against Babylon, there was a battle\\noutside the walls, and the gates of the city were opened to him\\n(538 B.C.).  ',\n",
              " 'His soldiers entered the city without fighting.  ',\n",
              " 'The\\ncrown prince Belshazzar, the son of Nabonidus, was feasting',\n",
              " ', the\\nBible relates, when a hand appeared and wrote in letters of fire\\nupon the wall these mystical words: _\"Mene, Mene, Tekel,\\nUpharsin,\"_ which was interpreted by the prophet Daniel, whom he\\nsummoned to read the riddle, as \"God has numbered thy kingdom and\\nfinished it; thou art weighed in the balance and found wanting and\\nthy kingdom is given to the Medes and Persians.',\n",
              " '\"  ',\n",
              " 'Possibly the\\npriests of Bel Marduk knew something about that writing on the\\nwall.  ',\n",
              " 'Belshazzar was killed that night, says the Bible.\\n',\n",
              " 'Nabonidus was taken prisoner, and the occupation of the city was\\nso peaceful that the services of Bel Marduk continued without\\nintermission.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVJF1L5PVyRi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "8a363a3d-dbac-4e5a-cb55-62fe5d93f431"
      },
      "source": [
        "words = [token.text for token in doc]\n",
        "words[:30]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Even',\n",
              " 'under',\n",
              " 'the',\n",
              " 'Assyrian',\n",
              " 'monarchs',\n",
              " 'and',\n",
              " 'especially',\n",
              " 'under',\n",
              " '\\n',\n",
              " 'Sardanapalus',\n",
              " ',',\n",
              " 'Babylon',\n",
              " 'had',\n",
              " 'been',\n",
              " 'a',\n",
              " 'scene',\n",
              " 'of',\n",
              " 'great',\n",
              " 'intellectual',\n",
              " '\\n',\n",
              " 'activity',\n",
              " '.',\n",
              " ' ',\n",
              " '{',\n",
              " '111',\n",
              " '}',\n",
              " 'Sardanapalus',\n",
              " ',',\n",
              " 'though',\n",
              " 'an']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xndApEFuVyRn",
        "colab_type": "text"
      },
      "source": [
        "### Filtering Tokens\n",
        "\n",
        "Let's start with cleaning the text and counting to see what we can learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSHaSQWqVyRo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "fdec8b77-7f20-4049-aaf1-3c21e510f8ad"
      },
      "source": [
        "# One of the common things we do in text analysis is to remove punctuation\n",
        "no_punct = [token for token in doc if token.is_punct == False]\n",
        "for token in no_punct[:20]:\n",
        "  print(token.text, token.is_punct)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Even False\n",
            "under False\n",
            "the False\n",
            "Assyrian False\n",
            "monarchs False\n",
            "and False\n",
            "especially False\n",
            "under False\n",
            "\n",
            " False\n",
            "Sardanapalus False\n",
            "Babylon False\n",
            "had False\n",
            "been False\n",
            "a False\n",
            "scene False\n",
            "of False\n",
            "great False\n",
            "intellectual False\n",
            "\n",
            " False\n",
            "activity False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K63rP_PJVyRs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "322949fb-aba2-4d98-dc9f-d8bd928881d1"
      },
      "source": [
        "# This has worked, but left in new line characters and spaces\n",
        "no_punct_or_space = [token for token in doc if token.is_punct == False and token.is_space == False]\n",
        "for token in no_punct_or_space[:30]:\n",
        "  print(token.text)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Even\n",
            "under\n",
            "the\n",
            "Assyrian\n",
            "monarchs\n",
            "and\n",
            "especially\n",
            "under\n",
            "Sardanapalus\n",
            "Babylon\n",
            "had\n",
            "been\n",
            "a\n",
            "scene\n",
            "of\n",
            "great\n",
            "intellectual\n",
            "activity\n",
            "111\n",
            "Sardanapalus\n",
            "though\n",
            "an\n",
            "Assyrian\n",
            "had\n",
            "been\n",
            "quite\n",
            "Babylon\n",
            "ized\n",
            "He\n",
            "made\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHjzgbbgVyRw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "789c1145-5e3c-4922-e683-50160aff4369"
      },
      "source": [
        "# Let's say we also want to remove numbers, and lowercase everything\n",
        "lower_alpha = [token.lower_ for token in no_punct_or_space if token.is_alpha == True]\n",
        "lower_alpha[:30]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['even',\n",
              " 'under',\n",
              " 'the',\n",
              " 'assyrian',\n",
              " 'monarchs',\n",
              " 'and',\n",
              " 'especially',\n",
              " 'under',\n",
              " 'sardanapalus',\n",
              " 'babylon',\n",
              " 'had',\n",
              " 'been',\n",
              " 'a',\n",
              " 'scene',\n",
              " 'of',\n",
              " 'great',\n",
              " 'intellectual',\n",
              " 'activity',\n",
              " 'sardanapalus',\n",
              " 'though',\n",
              " 'an',\n",
              " 'assyrian',\n",
              " 'had',\n",
              " 'been',\n",
              " 'quite',\n",
              " 'babylon',\n",
              " 'ized',\n",
              " 'he',\n",
              " 'made',\n",
              " 'a']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbpLhAqnVyR1",
        "colab_type": "text"
      },
      "source": [
        "One other common bit of preprocessing is to remove stopwords, that is, the common words in a language that don't convey the information that we are looking for in our analysis. For example, if we looked for the most common words in a text, we would want to remove stopwords so that we don't only get words such as 'a,' 'the,' and 'and.'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STyEpj96VyR2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "f5846d24-7541-4e6e-f33e-741245688f84"
      },
      "source": [
        "clean = [token.lower_ for token in no_punct_or_space if token.is_alpha == True and token.is_stop == False]\n",
        "clean[:30]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['assyrian',\n",
              " 'monarchs',\n",
              " 'especially',\n",
              " 'sardanapalus',\n",
              " 'babylon',\n",
              " 'scene',\n",
              " 'great',\n",
              " 'intellectual',\n",
              " 'activity',\n",
              " 'sardanapalus',\n",
              " 'assyrian',\n",
              " 'babylon',\n",
              " 'ized',\n",
              " 'library',\n",
              " 'library',\n",
              " 'paper',\n",
              " 'clay',\n",
              " 'tablets',\n",
              " 'writing',\n",
              " 'mesopotamia',\n",
              " 'early',\n",
              " 'sumerian',\n",
              " 'days',\n",
              " 'collection',\n",
              " 'unearthed',\n",
              " 'precious',\n",
              " 'store',\n",
              " 'historical',\n",
              " 'material',\n",
              " 'world']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWe736X7mwKF",
        "colab_type": "text"
      },
      "source": [
        "For this piece, we've used spaCy's built in stopword list, which is used to create the property `is_stop` for each token. There's a good chance you would want to create custom stopwords lists though, especially if you're working with historical text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP8b8upcmx5q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86b7c0fb-fb57-4a8e-bebf-bd8abf284274"
      },
      "source": [
        "# We'll just pick a couple of words we know are in the example\n",
        "custom_stopwords = [\"assyrian\", \"babylon\"]\n",
        "\n",
        "custom_clean = [token.lower_ for token in doc if token.lower_ not in custom_stopwords]\n",
        "custom_clean"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['even',\n",
              " 'under',\n",
              " 'the',\n",
              " 'monarchs',\n",
              " 'and',\n",
              " 'especially',\n",
              " 'under',\n",
              " '\\n',\n",
              " 'sardanapalus',\n",
              " ',',\n",
              " 'had',\n",
              " 'been',\n",
              " 'a',\n",
              " 'scene',\n",
              " 'of',\n",
              " 'great',\n",
              " 'intellectual',\n",
              " '\\n',\n",
              " 'activity',\n",
              " '.',\n",
              " ' ',\n",
              " '{',\n",
              " '111',\n",
              " '}',\n",
              " 'sardanapalus',\n",
              " ',',\n",
              " 'though',\n",
              " 'an',\n",
              " ',',\n",
              " 'had',\n",
              " 'been',\n",
              " 'quite',\n",
              " '\\n',\n",
              " '-',\n",
              " 'ized',\n",
              " '.',\n",
              " ' ',\n",
              " 'he',\n",
              " 'made',\n",
              " 'a',\n",
              " 'library',\n",
              " ',',\n",
              " 'a',\n",
              " 'library',\n",
              " 'not',\n",
              " 'of',\n",
              " 'paper',\n",
              " 'but',\n",
              " 'of',\n",
              " '\\n',\n",
              " 'the',\n",
              " 'clay',\n",
              " 'tablets',\n",
              " 'that',\n",
              " 'were',\n",
              " 'used',\n",
              " 'for',\n",
              " 'writing',\n",
              " 'in',\n",
              " 'mesopotamia',\n",
              " 'since',\n",
              " '\\n',\n",
              " 'early',\n",
              " 'sumerian',\n",
              " 'days',\n",
              " '.',\n",
              " ' ',\n",
              " 'his',\n",
              " 'collection',\n",
              " 'has',\n",
              " 'been',\n",
              " 'unearthed',\n",
              " 'and',\n",
              " 'is',\n",
              " '\\n',\n",
              " 'perhaps',\n",
              " 'the',\n",
              " 'most',\n",
              " 'precious',\n",
              " 'store',\n",
              " 'of',\n",
              " 'historical',\n",
              " 'material',\n",
              " 'in',\n",
              " 'the',\n",
              " '\\n',\n",
              " 'world',\n",
              " '.',\n",
              " ' ',\n",
              " 'the',\n",
              " 'last',\n",
              " 'of',\n",
              " 'the',\n",
              " 'chaldean',\n",
              " 'line',\n",
              " 'of',\n",
              " 'babylonian',\n",
              " 'monarchs',\n",
              " ',',\n",
              " '\\n',\n",
              " 'nabonidus',\n",
              " ',',\n",
              " 'had',\n",
              " 'even',\n",
              " 'keener',\n",
              " 'literary',\n",
              " 'tastes',\n",
              " '.',\n",
              " ' ',\n",
              " 'he',\n",
              " 'patronized',\n",
              " '\\n',\n",
              " 'antiquarian',\n",
              " 'researches',\n",
              " ',',\n",
              " 'and',\n",
              " 'when',\n",
              " 'a',\n",
              " 'date',\n",
              " 'was',\n",
              " 'worked',\n",
              " 'out',\n",
              " 'by',\n",
              " 'his',\n",
              " '\\n',\n",
              " 'investigators',\n",
              " 'for',\n",
              " 'the',\n",
              " 'accession',\n",
              " 'of',\n",
              " 'sargon',\n",
              " 'i',\n",
              " 'he',\n",
              " 'commemorated',\n",
              " 'the',\n",
              " '\\n',\n",
              " 'fact',\n",
              " 'by',\n",
              " 'inscriptions',\n",
              " '.',\n",
              " ' ',\n",
              " 'but',\n",
              " 'there',\n",
              " 'were',\n",
              " 'many',\n",
              " 'signs',\n",
              " 'of',\n",
              " 'disunion',\n",
              " 'in',\n",
              " '\\n',\n",
              " 'his',\n",
              " 'empire',\n",
              " ',',\n",
              " 'and',\n",
              " 'he',\n",
              " 'sought',\n",
              " 'to',\n",
              " 'centralize',\n",
              " 'it',\n",
              " 'by',\n",
              " 'bringing',\n",
              " 'a',\n",
              " 'number',\n",
              " 'of',\n",
              " '\\n',\n",
              " 'the',\n",
              " 'various',\n",
              " 'local',\n",
              " 'gods',\n",
              " 'to',\n",
              " 'and',\n",
              " 'setting',\n",
              " 'up',\n",
              " 'temples',\n",
              " 'to',\n",
              " 'them',\n",
              " '\\n',\n",
              " 'there',\n",
              " '.',\n",
              " ' ',\n",
              " 'this',\n",
              " 'device',\n",
              " 'was',\n",
              " 'to',\n",
              " 'be',\n",
              " 'practised',\n",
              " 'quite',\n",
              " 'successfully',\n",
              " 'by',\n",
              " 'the',\n",
              " '\\n',\n",
              " 'romans',\n",
              " 'in',\n",
              " 'later',\n",
              " 'times',\n",
              " ',',\n",
              " 'but',\n",
              " 'in',\n",
              " 'it',\n",
              " 'roused',\n",
              " 'the',\n",
              " 'jealousy',\n",
              " 'of',\n",
              " '\\n',\n",
              " 'the',\n",
              " 'powerful',\n",
              " 'priesthood',\n",
              " 'of',\n",
              " 'bel',\n",
              " 'marduk',\n",
              " ',',\n",
              " 'the',\n",
              " 'dominant',\n",
              " 'god',\n",
              " 'of',\n",
              " 'the',\n",
              " '\\n',\n",
              " 'babylonians',\n",
              " '.',\n",
              " ' ',\n",
              " 'they',\n",
              " 'cast',\n",
              " 'about',\n",
              " 'for',\n",
              " 'a',\n",
              " 'possible',\n",
              " 'alternative',\n",
              " 'to',\n",
              " '\\n',\n",
              " 'nabonidus',\n",
              " 'and',\n",
              " 'found',\n",
              " 'it',\n",
              " 'in',\n",
              " 'cyrus',\n",
              " 'the',\n",
              " 'persian',\n",
              " ',',\n",
              " 'the',\n",
              " 'ruler',\n",
              " 'of',\n",
              " 'the',\n",
              " '\\n',\n",
              " 'adjacent',\n",
              " 'median',\n",
              " 'empire',\n",
              " '.',\n",
              " ' ',\n",
              " 'cyrus',\n",
              " 'had',\n",
              " 'already',\n",
              " 'distinguished',\n",
              " 'himself',\n",
              " '\\n',\n",
              " 'by',\n",
              " 'conquering',\n",
              " 'croesus',\n",
              " ',',\n",
              " 'the',\n",
              " 'rich',\n",
              " 'king',\n",
              " 'of',\n",
              " 'lydia',\n",
              " 'in',\n",
              " 'eastern',\n",
              " 'asia',\n",
              " '\\n',\n",
              " 'minor',\n",
              " '.',\n",
              " ' ',\n",
              " '{',\n",
              " '112',\n",
              " '}',\n",
              " 'he',\n",
              " 'came',\n",
              " 'up',\n",
              " 'against',\n",
              " ',',\n",
              " 'there',\n",
              " 'was',\n",
              " 'a',\n",
              " 'battle',\n",
              " '\\n',\n",
              " 'outside',\n",
              " 'the',\n",
              " 'walls',\n",
              " ',',\n",
              " 'and',\n",
              " 'the',\n",
              " 'gates',\n",
              " 'of',\n",
              " 'the',\n",
              " 'city',\n",
              " 'were',\n",
              " 'opened',\n",
              " 'to',\n",
              " 'him',\n",
              " '\\n',\n",
              " '(',\n",
              " '538',\n",
              " 'b.c.',\n",
              " ')',\n",
              " '.',\n",
              " ' ',\n",
              " 'his',\n",
              " 'soldiers',\n",
              " 'entered',\n",
              " 'the',\n",
              " 'city',\n",
              " 'without',\n",
              " 'fighting',\n",
              " '.',\n",
              " ' ',\n",
              " 'the',\n",
              " '\\n',\n",
              " 'crown',\n",
              " 'prince',\n",
              " 'belshazzar',\n",
              " ',',\n",
              " 'the',\n",
              " 'son',\n",
              " 'of',\n",
              " 'nabonidus',\n",
              " ',',\n",
              " 'was',\n",
              " 'feasting',\n",
              " ',',\n",
              " 'the',\n",
              " '\\n',\n",
              " 'bible',\n",
              " 'relates',\n",
              " ',',\n",
              " 'when',\n",
              " 'a',\n",
              " 'hand',\n",
              " 'appeared',\n",
              " 'and',\n",
              " 'wrote',\n",
              " 'in',\n",
              " 'letters',\n",
              " 'of',\n",
              " 'fire',\n",
              " '\\n',\n",
              " 'upon',\n",
              " 'the',\n",
              " 'wall',\n",
              " 'these',\n",
              " 'mystical',\n",
              " 'words',\n",
              " ':',\n",
              " '_',\n",
              " '\"',\n",
              " 'mene',\n",
              " ',',\n",
              " 'mene',\n",
              " ',',\n",
              " 'tekel',\n",
              " ',',\n",
              " '\\n',\n",
              " 'upharsin',\n",
              " ',',\n",
              " '\"',\n",
              " '_',\n",
              " 'which',\n",
              " 'was',\n",
              " 'interpreted',\n",
              " 'by',\n",
              " 'the',\n",
              " 'prophet',\n",
              " 'daniel',\n",
              " ',',\n",
              " 'whom',\n",
              " 'he',\n",
              " '\\n',\n",
              " 'summoned',\n",
              " 'to',\n",
              " 'read',\n",
              " 'the',\n",
              " 'riddle',\n",
              " ',',\n",
              " 'as',\n",
              " '\"',\n",
              " 'god',\n",
              " 'has',\n",
              " 'numbered',\n",
              " 'thy',\n",
              " 'kingdom',\n",
              " 'and',\n",
              " '\\n',\n",
              " 'finished',\n",
              " 'it',\n",
              " ';',\n",
              " 'thou',\n",
              " 'art',\n",
              " 'weighed',\n",
              " 'in',\n",
              " 'the',\n",
              " 'balance',\n",
              " 'and',\n",
              " 'found',\n",
              " 'wanting',\n",
              " 'and',\n",
              " '\\n',\n",
              " 'thy',\n",
              " 'kingdom',\n",
              " 'is',\n",
              " 'given',\n",
              " 'to',\n",
              " 'the',\n",
              " 'medes',\n",
              " 'and',\n",
              " 'persians',\n",
              " '.',\n",
              " '\"',\n",
              " ' ',\n",
              " 'possibly',\n",
              " 'the',\n",
              " '\\n',\n",
              " 'priests',\n",
              " 'of',\n",
              " 'bel',\n",
              " 'marduk',\n",
              " 'knew',\n",
              " 'something',\n",
              " 'about',\n",
              " 'that',\n",
              " 'writing',\n",
              " 'on',\n",
              " 'the',\n",
              " '\\n',\n",
              " 'wall',\n",
              " '.',\n",
              " ' ',\n",
              " 'belshazzar',\n",
              " 'was',\n",
              " 'killed',\n",
              " 'that',\n",
              " 'night',\n",
              " ',',\n",
              " 'says',\n",
              " 'the',\n",
              " 'bible',\n",
              " '.',\n",
              " '\\n',\n",
              " 'nabonidus',\n",
              " 'was',\n",
              " 'taken',\n",
              " 'prisoner',\n",
              " ',',\n",
              " 'and',\n",
              " 'the',\n",
              " 'occupation',\n",
              " 'of',\n",
              " 'the',\n",
              " 'city',\n",
              " 'was',\n",
              " '\\n',\n",
              " 'so',\n",
              " 'peaceful',\n",
              " 'that',\n",
              " 'the',\n",
              " 'services',\n",
              " 'of',\n",
              " 'bel',\n",
              " 'marduk',\n",
              " 'continued',\n",
              " 'without',\n",
              " '\\n',\n",
              " 'intermission',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ENXxjBHVyR8",
        "colab_type": "text"
      },
      "source": [
        "At this point, we have a list of lower-cased tokens that doesn't contain punctuation, white-space, numbers, or stopwords. Depending on our analysis, we may or may not want to do this much cleaning. But, it is good to understand how much we can do just with spaCy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSGJfxiaVyR-",
        "colab_type": "text"
      },
      "source": [
        "### Counting Tokens\n",
        "\n",
        "Let's then look at what we can do now that we have groups of tokens at different lengths. We can start with just counting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEFjnPPLVySA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "40f882d0-4530-4f92-910e-5e9329920089"
      },
      "source": [
        "print(\"Number of tokens in document: \", len(doc))\n",
        "print(\"Number of tokens in cleaned document: \", len(clean))\n",
        "print(\"Number of unique tokens in cleaned document: \", len(set(clean)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tokens in document:  477\n",
            "Number of tokens in cleaned document:  175\n",
            "Number of unique tokens in cleaned document:  147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6JRDl22VySL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZwZ3En1VySY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "0075c981-a896-4fb5-c2de-73ecef18b6c9"
      },
      "source": [
        "full_counter = Counter([token.lower_ for token in doc])\n",
        "full_counter.most_common(20)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 36),\n",
              " ('\\n', 35),\n",
              " (',', 26),\n",
              " ('of', 20),\n",
              " ('.', 16),\n",
              " (' ', 14),\n",
              " ('and', 13),\n",
              " ('in', 9),\n",
              " ('a', 8),\n",
              " ('was', 8),\n",
              " ('to', 8),\n",
              " ('he', 6),\n",
              " ('by', 6),\n",
              " ('babylon', 5),\n",
              " ('had', 4),\n",
              " ('that', 4),\n",
              " ('his', 4),\n",
              " ('nabonidus', 4),\n",
              " ('it', 4),\n",
              " ('\"', 4)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIrMQFp6VySg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "a6483aa8-eb93-4335-f391-5e74c9eb3383"
      },
      "source": [
        "cleaned_counter = Counter(clean)\n",
        "cleaned_counter.most_common(20)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('babylon', 5),\n",
              " ('nabonidus', 4),\n",
              " ('bel', 3),\n",
              " ('marduk', 3),\n",
              " ('city', 3),\n",
              " ('assyrian', 2),\n",
              " ('monarchs', 2),\n",
              " ('sardanapalus', 2),\n",
              " ('library', 2),\n",
              " ('writing', 2),\n",
              " ('empire', 2),\n",
              " ('god', 2),\n",
              " ('found', 2),\n",
              " ('cyrus', 2),\n",
              " ('belshazzar', 2),\n",
              " ('bible', 2),\n",
              " ('wall', 2),\n",
              " ('mene', 2),\n",
              " ('thy', 2),\n",
              " ('kingdom', 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL9FAgJjVySu",
        "colab_type": "text"
      },
      "source": [
        "**Question:** Why do we have to use a list comprehension for the non-clean doc while we can just pass a variable directly for the cleaned set of tokens?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRNYHP7wVySv",
        "colab_type": "text"
      },
      "source": [
        "## Part-of-Speech Tagging\n",
        "\n",
        "Let's turn to the other aspects of the text that spaCy exposes for us. Depending on what questions we might have about the text, these will be more or less helpful. \n",
        "\n",
        "We'll start with parts of speech. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLVUUOT9VySw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "f4bd0e4d-1cb7-4394-ad00-05c5494668d0"
      },
      "source": [
        "# spaCy provides two levels of POS tagging. Here's the more general.\n",
        "for token in doc[:30]:\n",
        "  print(token.text, token.pos_)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Even ADV\n",
            "under ADP\n",
            "the DET\n",
            "Assyrian ADJ\n",
            "monarchs NOUN\n",
            "and CCONJ\n",
            "especially ADV\n",
            "under ADP\n",
            "\n",
            " SPACE\n",
            "Sardanapalus PROPN\n",
            ", PUNCT\n",
            "Babylon PROPN\n",
            "had VERB\n",
            "been VERB\n",
            "a DET\n",
            "scene NOUN\n",
            "of ADP\n",
            "great ADJ\n",
            "intellectual ADJ\n",
            "\n",
            " SPACE\n",
            "activity NOUN\n",
            ". PUNCT\n",
            "  SPACE\n",
            "{ PUNCT\n",
            "111 NUM\n",
            "} PUNCT\n",
            "Sardanapalus PROPN\n",
            ", PUNCT\n",
            "though ADP\n",
            "an DET\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbB2eeMTVyS2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "69ffc19a-a242-457b-f340-208b36c33238"
      },
      "source": [
        "# We also have the more specific Penn Treenbank tags.\n",
        "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
        "for token in doc[:30]:\n",
        "  print(token.text, token.tag_)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Even RB\n",
            "under IN\n",
            "the DT\n",
            "Assyrian JJ\n",
            "monarchs NNS\n",
            "and CC\n",
            "especially RB\n",
            "under IN\n",
            "\n",
            " _SP\n",
            "Sardanapalus NNP\n",
            ", ,\n",
            "Babylon NNP\n",
            "had VBD\n",
            "been VBN\n",
            "a DT\n",
            "scene NN\n",
            "of IN\n",
            "great JJ\n",
            "intellectual JJ\n",
            "\n",
            " _SP\n",
            "activity NN\n",
            ". .\n",
            "  _SP\n",
            "{ -LRB-\n",
            "111 CD\n",
            "} -RRB-\n",
            "Sardanapalus NNP\n",
            ", ,\n",
            "though IN\n",
            "an DT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6X3cbcmVyS4",
        "colab_type": "text"
      },
      "source": [
        "We can accumulate the groups of tokens by way of these in order understand distributions of parts of speech throughout the text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVey8EXsVyS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nouns = [token for token in doc if token.pos_ == \"NOUN\"]\n",
        "verbs = [token for token in doc if token.pos_ == \"VERB\"]\n",
        "proper_nouns = [token for token in doc if token.pos_ == \"PROPN\"]\n",
        "adjectives = [token for token in doc if token.pos_ == \"ADJ\"]\n",
        "adverbs = [token for token in doc if token.pos_ == \"ADV\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp6pH7VjVyTC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3275c7e2-ec18-44e0-c11e-b4dd3d6d039e"
      },
      "source": [
        "pos_counts = {\n",
        "    \"nouns\": len(nouns),\n",
        "    \"verbs\": len(verbs),\n",
        "    \"proper_nouns\": len(proper_nouns),\n",
        "    \"adjectives\": len(adjectives),\n",
        "    \"adverbs\": len(adverbs) \n",
        "}\n",
        "\n",
        "pos_counts"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'adjectives': 25, 'adverbs': 17, 'nouns': 66, 'proper_nouns': 46, 'verbs': 62}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_1y3C5LVyTP",
        "colab_type": "text"
      },
      "source": [
        "spaCy also provides full dependency parsing, but we're going to leave that alone for the moment. We'll turn instead to named entity recognition. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29Mqf_S0VyTR",
        "colab_type": "text"
      },
      "source": [
        "## Named-Entity Recognition\n",
        "\n",
        "https://spacy.io/api/annotation#named-entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KodfOLmHVyTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "outputId": "af4d69cc-2186-4768-a5ff-334b0cdee579"
      },
      "source": [
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Assyrian NORP\n",
            "Sardanapalus PERSON\n",
            "Babylon PERSON\n",
            "111 CARDINAL\n",
            "Sardanapalus PERSON\n",
            "Assyrian NORP\n",
            "Mesopotamia GPE\n",
            "early Sumerian days DATE\n",
            "Chaldean NORP\n",
            "Babylonian NORP\n",
            "Nabonidus PERSON\n",
            "Sargon PERSON\n",
            "Babylon PERSON\n",
            "the\n",
            "Romans ORG\n",
            "Babylon WORK_OF_ART\n",
            "Bel Marduk GPE\n",
            "Babylonians NORP\n",
            "Nabonidus PERSON\n",
            "Cyrus PERSON\n",
            "Persian NORP\n",
            "Median Empire GPE\n",
            "Cyrus ORG\n",
            "Croesus ORG\n",
            "Lydia in ORG\n",
            "Eastern Asia LOC\n",
            "112 CARDINAL\n",
            "Babylon ORG\n",
            "538 CARDINAL\n",
            "B.C. GPE\n",
            "Belshazzar PERSON\n",
            "Nabonidus PERSON\n",
            "Bible WORK_OF_ART\n",
            "Tekel GPE\n",
            "Upharsin PERSON\n",
            "Daniel PERSON\n",
            "Medes PERSON\n",
            "Persians NORP\n",
            "Bel Marduk GPE\n",
            "Belshazzar PERSON\n",
            "that night TIME\n",
            "Bible WORK_OF_ART\n",
            "Nabonidus PERSON\n",
            "Bel Marduk ORG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGCSdUMAVyTa",
        "colab_type": "text"
      },
      "source": [
        "What if we only care about geo-political entities or locations?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhIk-M0DVyTc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "bed0a206-4e18-43de-9fa0-961dbf1ae7d2"
      },
      "source": [
        "ent_filtered = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in [\"GPE\", \"LOC\"]]\n",
        "ent_filtered"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Mesopotamia', 'GPE'),\n",
              " ('Bel Marduk', 'GPE'),\n",
              " ('Median Empire', 'GPE'),\n",
              " ('Eastern Asia', 'LOC'),\n",
              " ('B.C.', 'GPE'),\n",
              " ('Tekel', 'GPE'),\n",
              " ('Bel Marduk', 'GPE')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buJBVUPQVyTe",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing Parses\n",
        "\n",
        "spaCy also has a nice built-in visualizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3Ra-HtPVyTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy import displacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIO_FEoLVyTi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "442ab825-73a1-457f-c101-0233fe1eb638"
      },
      "source": [
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Even under the \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Assyrian\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " monarchs and especially under</br>\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Sardanapalus\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Babylon\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " had been a scene of great intellectual</br>activity.  {\n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    111\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              "} \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Sardanapalus\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", though an \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Assyrian\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              ", had been quite</br>Babylon-ized.  He made a library, a library not of paper but of</br>the clay tablets that were used for writing in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Mesopotamia\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " since</br>\n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    early Sumerian days\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ".  His collection has been unearthed and is</br>perhaps the most precious store of historical material in the</br>world.  The last of the \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Chaldean\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " line of \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Babylonian\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " monarchs,</br>\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Nabonidus\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", had even keener literary tastes.  He patronized</br>antiquarian researches, and when a date was worked out by his</br>investigators for the accession of \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Sargon\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " I he commemorated the</br>fact by inscriptions.  But there were many signs of disunion in</br>his empire, and he sought to centralize it by bringing a number of</br>the various local gods to \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Babylon\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " and setting up temples to them</br>there.  This device was to be practised quite successfully by \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    the\n",
              "Romans\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " in later times, but in \n",
              "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Babylon\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
              "</mark>\n",
              " it roused the jealousy of</br>the powerful priesthood of \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Bel Marduk\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", the dominant god of the</br>\n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Babylonians\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              ".  They cast about for a possible alternative to</br>\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Nabonidus\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " and found it in \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Cyrus\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " the \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Persian\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              ", the ruler of the</br>adjacent \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Median Empire\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ".  \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Cyrus\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " had already distinguished himself</br>by conquering \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Croesus\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", the rich king of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Lydia in\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Eastern Asia\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              "</br>Minor.  {\n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    112\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              "} He came up against \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Babylon\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", there was a battle</br>outside the walls, and the gates of the city were opened to him</br>(\n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    538\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    B.C.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ").  His soldiers entered the city without fighting.  The</br>crown prince \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Belshazzar\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", the son of \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Nabonidus\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", was feasting, the</br>\n",
              "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Bible\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
              "</mark>\n",
              " relates, when a hand appeared and wrote in letters of fire</br>upon the wall these mystical words: _&quot;Mene, Mene, \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Tekel\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ",</br>\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Upharsin\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ",&quot;_ which was interpreted by the prophet \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Daniel\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", whom he</br>summoned to read the riddle, as &quot;God has numbered thy kingdom and</br>finished it; thou art weighed in the balance and found wanting and</br>thy kingdom is given to the \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Medes\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Persians\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              ".&quot;  Possibly the</br>priests of \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Bel Marduk\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " knew something about that writing on the</br>wall.  \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Belshazzar\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " was killed \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    that night\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
              "</mark>\n",
              ", says the \n",
              "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Bible\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
              "</mark>\n",
              ".</br>\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Nabonidus\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " was taken prisoner, and the occupation of the city was</br>so peaceful that the services of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Bel Marduk\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " continued without\n",
              "intermission.</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU5dIAnMiODg",
        "colab_type": "text"
      },
      "source": [
        "### Activity\n",
        "\n",
        "Pick either a particular part of speech or a named entity type, and write code to determine the most common words of that type. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mro3MhI-ieQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F5P7cbMVyTl",
        "colab_type": "text"
      },
      "source": [
        "# Corpus-level Analysis with `textacy`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCwkgf9pVyTl",
        "colab_type": "text"
      },
      "source": [
        "Let's shift to thinking about a whole corpus rather than a single document.\n",
        "\n",
        "In doing so, we could keep working with spaCy directly if the features that it exposes help us answer the research questions we are asking. \n",
        "\n",
        "Instead, though, we're going to take advantage of textacy, a library built on spaCy that adds features, including a sense of a Corpus and built in analytics on it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp3AcJezVyTn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "8e5529e9-a200-4851-a204-b73bc18ba868"
      },
      "source": [
        "!pip install textacy"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/5e/3b8391cf6ff39350b73f8421184cf6792002b5c2c17982b7c9fbd5ff36de/textacy-0.9.1-py3-none-any.whl (203kB)\n",
            "\r\u001b[K     |█▋                              | 10kB 18.6MB/s eta 0:00:01\r\u001b[K     |███▏                            | 20kB 6.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 30kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 40kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 51kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 61kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 71kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 81kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 92kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 102kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 112kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 122kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 133kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 143kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 153kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 163kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 174kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 184kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 194kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.3.1)\n",
            "Collecting jellyfish>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/80/bcacc7affb47be7279d7d35225e1a932416ed051b315a7f9df20acf04cbe/jellyfish-0.7.2.tar.gz (133kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 56.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from textacy) (3.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.4)\n",
            "Requirement already satisfied: pyemd>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.5.1)\n",
            "Collecting cytoolz>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/b1/7f16703fe4a497879b1b457adf1e472fad2d4f030477698b16d2febf38bb/cytoolz-0.10.1.tar.gz (475kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 43.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.21.3)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.6/dist-packages (from textacy) (4.28.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.21.0)\n",
            "Requirement already satisfied: srsly>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.2.0)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.14.0)\n",
            "Requirement already satisfied: spacy>=2.0.12 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.1.9)\n",
            "Collecting pyphen>=0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/82/08a3629dce8d1f3d91db843bb36d4d7db6b6269d5067259613a0d5c8a9db/Pyphen-0.9.5-py2.py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.17.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->textacy) (4.4.1)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz>=0.8.0->textacy) (0.10.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (1.24.3)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (2.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (1.0.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (7.0.8)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (2.0.1)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (0.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (0.3.0)\n",
            "Building wheels for collected packages: jellyfish, cytoolz\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.7.2-cp36-cp36m-linux_x86_64.whl size=72991 sha256=433e7cd4b1a50e711faafb9bf480e73867c4af9a60fe872ccc8d55d2fb44d437\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/fe/99/d8fa8f2ef7b82a625b0b77a84d319b0b50693659823c4effb4\n",
            "  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cytoolz: filename=cytoolz-0.10.1-cp36-cp36m-linux_x86_64.whl size=1256692 sha256=aa712d161710c7c443b1d2e9f0b6ae99503c2237f5d77489a69b19f44e7fac17\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/2a/18/d962b614e055577e7d9a3e4813e0742f822ca9c8800cc3783a\n",
            "Successfully built jellyfish cytoolz\n",
            "Installing collected packages: jellyfish, cytoolz, pyphen, textacy\n",
            "Successfully installed cytoolz-0.10.1 jellyfish-0.7.2 pyphen-0.9.5 textacy-0.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMqTW64-VyTp",
        "colab_type": "text"
      },
      "source": [
        "## Generating Corpora\n",
        "\n",
        "We'll use some of the data that is included in textacy as our corpus. You could absolutely import data otherwise, whether through reading in plain text or xml files, or pulling text data and metadata from a csv file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8HmDN36VyTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import textacy\n",
        "import textacy.datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYKe7-BCVyTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We'll work with some Supreme Court cases: https://chartbeat-labs.github.io/textacy/_modules/textacy/datasets/supreme_court.html\n",
        "data = textacy.datasets.SupremeCourt()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xmidNbxVyTu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8597b084-f196-491d-f7e1-5729cab02050"
      },
      "source": [
        "data.download()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 111M/111M [00:04<00:00, 25.8MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9X-twUBVyTw",
        "colab_type": "text"
      },
      "source": [
        "What we have here is a collection of Supreme Court decisions, both full text and metadata. \n",
        "\n",
        "Let's look at a single one to see what we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHMbaig9VyTx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "27cd1cb7-e35b-4899-f8f6-b877cbccc6d6"
      },
      "source": [
        "single = list(data.texts(limit=1))[0]\n",
        "single[:200]"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[ Halliburton Oil Well Cementing Co. v. Walker Mr.Earl Babcock, of Duncan, Okl. (Harry C. Robb, of Washington, D.C., on the brief), for petitioner.\\n Mr. Harold W. Mattingly, of Los Angeles, Cal., for '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4Xf3CkM9dJS",
        "colab_type": "text"
      },
      "source": [
        "Let's go ahead and pull a full set of texts with metadata. To keep it a bit more manageable time-wise, we'll only collect 100 of the records."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZk5b7zxVyTz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "70449ffc-59a1-4e1f-b9d7-464d8549137c"
      },
      "source": [
        "records = data.records(limit=100)\n",
        "\n",
        "# Records here is a generator - we can look at the first record by passing it to the next function.\n",
        "next(records)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[ Halliburton Oil Well Cementing Co. v. Walker Mr.Earl Babcock, of Duncan, Okl. (Harry C. Robb, of Washington, D.C., on the brief), for petitioner.\\n Mr. Harold W. Mattingly, of Los Angeles, Cal., for respondents.\\n Mr. Justice BLACK delivered the opinion of the Court.\\n Cranford P. Walker, owner of Patent No. 2,156,519, and the other respondents, licensees under the patent, brought this suit in a federal district court alleging that petitioner, Halliburton Oil Well Cementing Company, had infringed certain of the claims of the Walker patent. The district court held the claims in issue valid and infringed by Halliburton. The circuit court of appeals affirmed, 9 Cir., 146 F.2d 817, and denied Halliburton\\'s petition for rehearing. 149 F.2d 896. Petitioner\\'s application to this Court for certiorari urged, among other grounds, that the claims held valid failed to make the \\'full, clear, concise, and exact\\' description of the alleged invention required by Rev.Stat. 4888C. 33, 35 U.S.C.A. 33,1 as that statute was interpreted by us in General Electric Co. v. Wabash Appliance Corporation, .2 This statutory requirement of distinctness and certainty in claims is important in patent law. We granted certiorari to consider whether it was correctly applied in this case. .3\\n The patent in suit was sustained as embodying an improvement over a past patent of Lehr and Wyatt (No. 2,047,974) upon an apparatus designed to facilitate the pumping of oil out of wells which do not have sufficient natural pressures to force the oil to gush. An outline of the background and setting of these patents is helpful to an understanding of the problem presented.\\n In order to operate a pump in an oil well most efficiently, cheaply, and with the least waste, the pump must be placed in an appropriate relationship to the fluid surface of the oil. Properly to place the pump in this relationship requires knowledge of the distance from the well top to the fluid surface. At least by the latter 1920\\'s problems of waste and expense in connection with non-gusher oil wells pressed upon the industry. See Railroad Commission of Texas v. Rowan & Nichols Oil Co., ; Burford v. Sun Oil Co., . It became apparent that inefficient pumping, one cause of waste, was in some measure attributable to lack of accurate knowledge of distance from well top to fluid surface. Ability to measure this distance in each separate non-gusher oil well became an obvious next step in the solution of this minor aspect of the problem of waste.\\n The surface and internal machinery and the corkscrew conformation of some oil wells make it impractical to measure depth by the familiar method of lowering a rope or cable. In casting about for an alternative method it was quite natural to hit upon the possibility of utilizing a sound-echo- time method. Unknown distances had frequently been ascertained by this method. Given the time elapsing between the injection of a sound into an oil well and the return of its echo from the fluid surface, and assuming the velocity of the sound to be about 1,100 feet per second, as it is in the open air, it would be easy to find the distance. Not only had this sound-echo-time method been long known and generally used to find unknown distances, but in 1898 Batcheller, in Patent No. 602,422, had described an apparatus to find a distance in a tubular space. Obviously an oil well is such a space. He described a device whereby the noise from a gun might be injected into a tube; the returning echoes from obstructions agitated a diaphragm, which in turn moved a stylus. The stylus recorded on a piece of paper a graph or diagram showing the variant movements of the diaphragm caused by its response to all the different echo waves.\\n In the late 1920\\'s the oil industry began to experiment in the use of this same sound-echo-time method for measur- ing the distance to the fluid surface in deep oil wells. A product of this experimentation was the Lehr and Wyatt patent, upon which the present patent claims to be an improvement. It proposed to measure the distance by measuring the time of travel of the echo of \\'an impulse wave\\' generated by a \\'sudden change in pressure.\\' The apparatus described included a gas cylinder with a quick operating valve by means of which a short blast of gas could be injected into a well. It was stated in the patent that the time elapsing between the release of the gas and the return of the echo of the waves produced by it could be observed in any desired manner. But the patentee\\'s application and drawings noted that the wave impulses could be recorded by use of a microphone which might include an amplifier and an appropriate device to record a picture of the wave impulses.\\n This Lehr and Wyatt patent, it is therefore apparent, simply provided an apparatus composed of old and well-known devices to measure the time required for pressure waves to move to and back from the fluid surface of an oil well. But the assumption that sound and pressure waves would travel in oil wells at open-air velocity of 1,100 feet per second proved to be erroneous. For this reason the timevelocity computation of Lehr and Wyatt for measuring the distance to the fluid surface produced inaccurate results.\\n After conferences with Lehr, Walker undertook to search for a method which would more accurately indicate the sound and pressure wave velocity in each well. Walker was familiar with the structure of oil wells. The oil flow pipe in a well, known as a tubing string, is jointed and where these joints occur there are collars or shoulders. There are also one or more relatively prominent projections on the oil flow pipe known as tubing catchers. In wells where the distance to the tubing catcher is known, Walker observed that the distance to the fluid surface could be measured by a simple time-distance proportion formula. 4\\n For those wells in which the distance to the tubing catcher was unknown, Walker also suggested another idea. The sections of tubing pipe used in a given oil well are generally of equal length. Therefore the shoulders in a given well ordinarily are at equal intervals from each other. But the section length and therefore the interval may vary from well to well. Walker concluded that he could measure the unknown distance to the tubing catcher if he could observe and record the shoulder echo waves. Thus multiplication of the number of shoulders observed by the known length of a pipe section would produce the distance to the tubing catcher. With this distance, he could solve the distance to the fluid surface by the same proportion formula used when the distance to the tubing catcher was a matter of record. The Lehr and Wyatt instrument could record all these echo waves. But the potential usefulness of the echoes from the shoulders and the tubing catcher which their machine recorded had not occurred to Lehr and Wyatt and consequently they had made no effort better to observe and record them. Walker\\'s contribution which he claims to be invention was in effect to add to Lehr and Wyatt\\'s apparatus a well-known device which would make the regularly appearing shoulder echo waves more prominent on the graph and easier to count.\\n The device added was a mechanical acoustical resonator. This was a short pipe which would receive wave impulses at the mouth of the well. Walker\\'s testimony was, and his specifications state, that by making the length of this tubal resonator one-third the length of the tubing joints, the resonator would serve as a tuner, adjusted to the frequency of the shoulder echo waves. It would simultaneously amplify these echo waves and eliminate unwanted echoes from other obstructions thus producing a clearer picture of the shoulder echo waves. His specifications show, attached to the tubal resonator, a coupler, the manipulation of which would adjust the length of the tube to one-third of the interval between shoulders in a particular well. His specifications and drawings also show the physical structure of a complete apparatus, designed to inject pressure impulses into a well, and to receive, note, record and time the impulse waves.\\n The District Court held the claims here in suit valid upon its finding that Walker\\'s \\'apparatus differs from and is an improvement over the prior art in the incorporation in such apparatus of a tuned acoustical means which performs the functions of a sound filter ....\\' The circuit court of appeals affirmed this holding, stating that the trial court had found \\'that the only part of this patent constituting invention over the prior art is the \\'tuned acoustical means which performs the functions of a sound filter.\"\\n For our purpose in passing upon the sufficiency of the claims against prohibited indefiniteness we can accept without ratifying the findings of the lower court that the addition of \\'(a) tuned acoustical means\\' performing the \\'functions of a sound filter\\' brought about a new patentable combination, even though it advanced only a narrow step beyond Lehr and Wyatt\\'s old combination. 5 We must, however, determine whether, as petitioner charges, the claims here held valid run afoul of Rev.Stat. 4888 because they do not describe the invention but use \\'conveniently functional language at the exact point of novelty.\\' General Electric Co. v. Wabash Appliance Corporation supraat page 371, 58 S.Ct. at page 903\\n Walker, in some of his claims, e.g., claims 2 and 3, does describe the tuned acoustical pipe as an integral part of his invention, showing its structure, its working arrangement in the alleged new combination, and the manner of its connection with the other parts. But no one of the claims on which this judgment rests has even suggested the physical structure of the acoustical resonator. 6\\n No one of these claims describes the physical relation of the Walker addition to the old Lehr and Wyatt machine. No one of these claims describes the manner in which the Walker addition will operate together with the old Lehr and Wyatt machine so as to make the \\'new\\' unitary apparatus perform its designed function. Thus the claims failed adequately to depict the structure, mode, and operation of the parts in combination.\\n A claim typical of all of those held valid only describes the resonator and its relation with the rest of the apparatus as \\'means associated with said pressure responsive device for tuning said receiving means to the frequency of echoes from the tubing collars of said tubing section to clearly distinguish the echoes of said couplings from each other.\\' 7\\n The language of the claim thus describes this most crucial element in the \\'new\\' combination in terms of what it will do rather than in terms of its own physical characteristics or its arrangement in the new combination apparatus. We have held that a claim with such a description of a product is invalid as a violation of Rev.Stat. 4888. Holland Furniture Co. v. Perkins Glue Co., , 257 S., 48 S.Ct. 474, 478, 479; General Electric Co. v. Wabash Appliance Corporation, supra. We understand that the circuit court of appeals held that the same rigid standards of description required for product claims is not required for a combination patent embodying old elements only. We have a different view.\\n Rev.Stat. 4888 pointedly provides that \\'in the case of a machine, he (the patentee) shall explain the principle thereof, and the best mode in which he has contemplated applying that principle, so as to distinguish it from other inventions; and he shall particularly point out and distinctively claim the part, improvement, or combination which he claims as his invention or discovery.\\' It has long been held that the word \\'machine\\' includes a combination. Corning et al. v. Burden, 15 How. 252, 267. We are not persuaded that the public and those affected by patents should lose the protection of this statute merely because the patented device is a combination of old elements.\\n Patents on machines which join old and well-known devices with the declared object of achieving new results, or patents which add an old element to improve a preexisting combination, easily lend themselves to abuse. And to prevent extension of a patent\\'s scope beyond what was actually invented, courts have viewed claims to combinations and improvements or additions to them with very close scrutiny. Cf. Lincoln Engineering Co. of Illinois v. Stewart Warner Corporation, , 549-551, 58 S.Ct. 662. For the same reason, courts have qualified the scope of what is meant by the equivalent of an ingredient of a combination of old elements. Gill v. Wells, 22 Wall. 1, 28, 29. Fuller v. Yentzer, , 298 S.. It is quite consistent with this strict interpretation of patents for machines which combine old elements to require clear description in combination claims. This view, clearly expressed in Gill v. Wells, supra, is that\\n\\'Where the ingredients are all old the invention ... consists entirely in the combination, and the requirement of the Patent Act that the invention shall be fully and exactly described applies with as much force to such an invention as to any other class, because if not fulfilled all three of the great ends intended to be accomplished by that requirement would be defeated. ... (1.) That the Government may know what they have granted and what will become public property when the term of the monopoly expires. (2.) That licensed persons desiring to practice the invention may know, during the term, how to make, construct, and use the invention. (3.) That other inventors may know what part of the field of invention is unoccupied.\\n\\'Purposes such as these are of great importance in every case, but the fulfillment of them is never more necessary than when such inquiries arise in respect to a patent for a machine which consists of a combination of old ingredients. Patents of that kind are much more numerous than any other, and consequently it is of the greatest importance that the description of the combination, which is the invention, should be full, clear, concise, and exact.\\' Gill v. Wells, supra, 22 Wall. at pages 25, 26.\\n These principles were again emphasized in Merrill v. Yeomans, , 570, where it was said that \\'... in cases where the invention is a new combination of old devices, he (the patentee) is bound to describe with particularity all these old devices, and then the new mode of combining them, for which he desires a patent.\\' This view has most recently been reiterated in General Electric Co. v. Wabash Appliance Corporation, supraat pages 368, 369, 58 S.Ct. at pages 901, 902. Cogent reasons would have to be presented to persuade us to depart from this established doctrine. The facts of the case before us, far from undermining our confidence in these earlier pronouncements, reinforce the conclusion that the statutory requirement for a clear description of claims applies to a combination of old devices.\\n This patent and the infringement proceedings brought under it illustrate the hazards of carving out an exception to the sweeping demand Congress made in Rev.Stat. 4888. Neither in the specification, the drawing, nor in the claims here under consideration, was there any indication that the patentee contemplated any specific structural alternative for the acoustical resonator or for the resonator\\'s relationship to the other parts of the machine. Petitioner was working in a field crowded almost, if not completely, to the point of exhaustion. In 1920, Tucker, in Patent No. 1,451,356, had shown a tuned acoustical resonator in a sound detecting device which measured distances. Lehr and Wyatt had provided for amplification of their waves. Sufficient amplification and exaggeration of all the differ- ent waves which Lehr and Wyatt recorded on their machine would have made it easy to distinguish the tubing catcher and regular shoulder waves from all others. For, even without this amplification, the echo waves from tubing collars could by proper magnification have been recorded and accurately counted, had Lehr and Wyatt recognized their importance in computing the velocity. Cf. General Electric Co. v. Jewel Incandescent Lamp Co., .\\n Under these circumstances the broadness, ambiguity, and overhanging threat of the functional claim of Walker become apparent. What he claimed in the court below and what he claims here is that his patent bars anyone from using in an oil well any device heretofore or hereafter invented which combined with the Lehr and Wyatt machine performs the function of clearly and distinctly catching and recording echoes from tubing joints with regularity. Just how many different devices there are of various kinds and characters which would serve to emphasize these echoes, we do not know. The Halliburton device, alleged to infringe, employs an electric filter for this purpose. In this age of technological development there may be many other devices beyond our present information or indeed our imagination which will perform that function and yet fit these claims. And unless frightened from the course of experimentation by broad functional claims like these, inventive genius may evolve many more devices to accomplish the same purpose. See United Carbon Co. et al. v. Binney & Smith Co., ; Burr v. Duryee, 1 Wall. 531, 568; O\\'Reilly et al. v. Morse et al., 15 How. 62, 112, 113. Yet if Walker\\'s blanket claims be valid, no device to clarify echo waves, now known or hereafter invented, whether the device be an actual equivalent of Walker\\'s ingredient or not, could be used in a combination such as this, during the life of Walker\\'s patent. Had Walker accurately described the machine he claims to have invented, he would have had no such broad rights to bar the use of all devices now or hereafter known which could accent waves. For had he accurately described the resonator together with the Lehr and Wyatt apparatus, and sued for infringement, charging the use of something else used in combination to accent the waves, the alleged infringer could have prevailed if the substituted device (1) performed a substantially different function; (2) was not known at the date of Walker\\'s patent as a proper substitute for the resonator; or (3) had been actually invented after the date of the patent. Fuller v. Yentzer, supraat pages 296, 297; Gill v. Wells, supra, 22 Wall. at page 29. Certainly, if we are to be consistent with Rev.Stat. 4888, a patentee cannot obtain greater coverage by failing to describe his invention than by describing it as the statute commands.\\n It is urged that our conclusion is in conflict with the decision of Continental Paper Bag Co. v. Eastern Paper Bag Co., . In that case, however, the claims structurally described the physical and operating relationship of all the crucial parts of the novel combination. 8\\n The court there decided only that there had been an infringement of this adequately described invention. That case is not authority for sustaining the claims before us which fail adequately to describe the alleged invention.\\nREVERSED.\\n Mr. Justice FRANKFURTER concurs with the Court\\'s opinion in so far as it finds this claim lacking in the definiteness required by Rev.Stat. 4888, 35 U.S.C. 33, 35 U.S.C.A. 33, but reserves judgment as to considerations that may be peculiar to combination patents in satisfying that requirement.\\n Mr. Justice BURTON dissents. Footnotes\\n[\\nFootnote 1\\n] \\'33. Application for Patent; Description; Specification and Claim. Before any inventor or discoverer shall receive a patent for his invention or discovery he shall make application therefor, in writing, to the Commissioner of Patents, and shall file in the Patent Office a written description of the same, and of the manner and process of making, constructing, compounding, and using it, in such full, clear, concise, and exact terms as to enable any person skilled in the art or science to which it appertains, or with which it is most nearly connected, to make, construct, compound, and use the same; and in case of a machine, he shall explain the principle thereof, and the best mode in which he has contemplated applying that principle, so as to distinguish it from other inventions; and he shall particularly point out and distinctly claim the part, improvement, or combination which he claims as his invention or discovery. ...\\' [\\nFootnote 2\\n] Other alleged errors were urged in the application for certiorari and have been argued here, but since we find the question of definiteness of the claim decisive of the controversy, we shall not further advert to the other contentions. [\\nFootnote 3\\n] This case was previously affirmed by a divided court, and upon petition for rehearing was restored to the docket for reargument. . [\\nFootnote 4\\n] The known distance from well top to the tubing catcher is to the unknown distance from well top to the fluid surface as the time an echo requires to travel from the tubing catcher is to the time required for an echo to travel from the fluid surface.\\n Walker\\'s patent emphasizes that his invention solves the velocity of sound waves in wells of various pressures in which sound did not travel at open-air or a uniform speed. Mathematically, of course, his determination of the distance by proportions determines the distance to the fluid surface directly without necessarily considering velocity in feet per second as a factor. [\\nFootnote 5\\n] See Hailes v. Van Wormer, 20 Wall. 353; Knapp v. Morss, , 228 S., 14 S.Ct. 81, 83, 84; Textile Machine Works v. Louis Hirsch Textile Machines, Inc., , 58 S. Ct. 291; Lincoln Engineering Co. of Illinois v. Stewart- Warner Corp., , 550 S., 58 S.Ct. 662, 664, 665. [\\nFootnote 6\\n] Halliburton does not challenge the adequacy of the description of any other features of the \\'new combination.\\' The elements of Walker\\'s apparatus other than the filter are so nearly identical to what Lehr and Wyatt patented that we can speak of these other elements as the \\'Lehr and Wyatt machine.\\' [\\nFootnote 7\\n] Both parties have used Claim 1 as a typical example for purposes of argument throughout the litigation. Other claims need not be set out. Claim 1 is as follows: \\'In an apparatus for determining the location of an obstruction in a well having therein a string of assembling tubing sections inter-connected with each other by coupling collars, means communicating with said well for creating a pressure impulse in said well, echo receiving means including a pressure responsive device exposed to said well for receiving pressure impulses from the well and for measuring the lapse of time between the creation of the impulse and the arrival at said receiving means of the echo from said obstruction, and means associated with said pressure responsive device for tuning said receiving means to the frequency of echoes from the tubing collars of said tubing sections to clearly distinguish the echoes from said couplings from each other.\\' [\\nFootnote 8\\n] The typical claim there in suit was as follows: \\'2. In a paper bag machine, the combination of the rotating cylinder provided with one or more pairs of sidefolding fingers adapted to be moved toward or from each other, a forming plate also provided with side-forming fingers adapted to be moved toward or from each other, means for operating said fingers at definite times during the formative action upon the bag tube, operating means for the forming plate adapted to cause the said plate to oscillate about its rear edge upon the surface of the cylinder during the rotary movement of said cylinder for the purpose of opening and forming the bottom of the bag tube, a finger moving with the forming plate for receiving the upper sheet of the tube and lifting it during the formative action, power devices for returning the forming plate to its original position to receive a new bag tube, and means to move the bag tube with the cylinder.\\' Continental Paper Bag Co. v. Eastern Paper Bag Co., , 417, n. 1, 28 S.Ct. 748, 750.',\n",
              " {'argument_date': '1946-01-09',\n",
              "  'case_name': 'HALLIBURTON OIL WELL CEMENTING CO. v. WALKER et al., DOING BUSINESS AS DEPTHOGRAPH CO.',\n",
              "  'decision_date': '1946-11-18',\n",
              "  'decision_direction': 'liberal',\n",
              "  'issue': '80180',\n",
              "  'issue_area': 8,\n",
              "  'maj_opinion_author': 78,\n",
              "  'n_maj_votes': 8,\n",
              "  'n_min_votes': 1,\n",
              "  'us_cite_id': '329 U.S. 1'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TwurMgHVyT0",
        "colab_type": "text"
      },
      "source": [
        "textacy includes the idea of a corpus, while spaCy only has an idea of a single documents, though you can compose documents in standard Python data structures. Every corpus takes some texts or text plus metadata, along with a language model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gdHIQAyVyT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = textacy.Corpus(nlp, data=records)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_0YOC-kVyT3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "665bc58c-4e0d-4a8c-facf-aad2ef64327c"
      },
      "source": [
        "corpus"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Corpus(99 docs, 610554 tokens)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBnvE5ZJVyT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "1ab4616d-64ff-4d7c-e344-a5f6e0c855af"
      },
      "source": [
        "[doc._.preview for doc in corpus[:5]]"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Doc(5222 tokens: \"Rehearing Denied Dec. 16, 1946. See . Mr.Claude...\")',\n",
              " 'Doc(3616 tokens: \"Rehearing Denied Dec. 16, 1946  See .  Appeal f...\")',\n",
              " 'Doc(9169 tokens: \"Mr. Walter J. Cummings, Jr., of Washington, D.C...\")',\n",
              " 'Doc(1423 tokens: \"Mr.A. Devitt Vaneck, of Washington, D.C., for p...\")',\n",
              " 'Doc(7624 tokens: \"Action by Richfield Oil Corporation against Sta...\")']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuBjFKQ4VyT8",
        "colab_type": "text"
      },
      "source": [
        "We can see that the type of each item in the corpus is a `Doc` - this is effectively a spaCy doc with all of the calculated features. Textacy does give you some capacity to work with those features through it's API, and also exposes new features, such as ngrams and ranking algorithms for single documents. We'll come back to these once we work a bit at the corpus level. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DFWqESvVyT8",
        "colab_type": "text"
      },
      "source": [
        "We can filter this corpus based on metadata once we make it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-CqNgQaVyUC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a7792c5-27da-4864-cf6f-2d704bdaf7fd"
      },
      "source": [
        "# Here we'll find all the cases where the number of justices voting in the majority was greater than 6. \n",
        "recent = [doc for doc in corpus.get(lambda doc: doc._.meta[\"n_maj_votes\"] > 6)]\n",
        "len(recent)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o5hy0pCVyUG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60aa4a4f-6d19-4dac-c27a-d68d72fd358c"
      },
      "source": [
        "recent[0]._.preview"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Doc(7624 tokens: \"Action by Richfield Oil Corporation against Sta...\")'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8YO2bgIVyUM",
        "colab_type": "text"
      },
      "source": [
        "## Analyzing the Corpus\n",
        "\n",
        "Let's look at what we get out of the box from textacy once we've built a corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wOMpBE0VyUM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "2bd3a5c4-e14f-4213-f8e5-984515bc0f41"
      },
      "source": [
        "print(\"number of documents: \", corpus.n_docs)\n",
        "print(\"number of sentences: \", corpus.n_sents)\n",
        "print(\"number of tokens: \", corpus.n_tokens)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of documents:  99\n",
            "number of sentences:  27826\n",
            "number of tokens:  610554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiQQrx5SVyUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We'll pass as_strings so that the results we look at will give us strings rather than unique ids.\n",
        "counts = corpus.word_counts(as_strings=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWwr9r1fFOuE",
        "colab_type": "text"
      },
      "source": [
        "Notice that, by default, the `word_counts` function is doing a certain amount of cleaning for you: https://chartbeat-labs.github.io/textacy/api_reference/lang_doc_corpus.html#textacy.corpus.Corpus.word_counts "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blFXigFMVyUQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "2d535bbe-8be1-4f15-96fe-acc4ae63cc0e"
      },
      "source": [
        "sorted(counts.items(), key=lambda x: x[1], reverse=True)[:20]"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('-PRON-', 15174),\n",
              " ('v.', 3067),\n",
              " ('Court', 2149),\n",
              " ('case', 2105),\n",
              " ('court', 1764),\n",
              " ('Act', 1738),\n",
              " ('States', 1669),\n",
              " ('state', 1668),\n",
              " ('United', 1606),\n",
              " ('footnote', 1576),\n",
              " ('Co.', 1226),\n",
              " ('order', 1119),\n",
              " ('law', 1093),\n",
              " ('Commission', 1028),\n",
              " ('right', 885),\n",
              " ('Congress', 881),\n",
              " ('power', 823),\n",
              " ('act', 815),\n",
              " ('tax', 788),\n",
              " ('employee', 764)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYx5VX51H8SF",
        "colab_type": "text"
      },
      "source": [
        "For an explanation of `-PRON-`, see https://spacy.io/api/annotation#lemmatization. Basically it's spaCy's way of lemmatizing pronouns. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELd2rxUZVyUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_doc_counts = corpus.word_doc_counts(weighting=\"freq\", smooth_idf=True, filter_stops=True, as_strings=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIIkImS2VyUW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "fba115c1-e80c-460c-8413-20994e489133"
      },
      "source": [
        "sorted(word_doc_counts.items(), key=lambda x:x[1], reverse=True)[:20]"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Mr.', 1.0),\n",
              " ('Court', 1.0),\n",
              " ('-PRON-', 1.0),\n",
              " ('opinion', 0.98989898989899),\n",
              " ('case', 0.98989898989899),\n",
              " ('v.', 0.98989898989899),\n",
              " ('Justice', 0.9696969696969697),\n",
              " ('deliver', 0.9696969696969697),\n",
              " ('fact', 0.9494949494949495),\n",
              " ('hold', 0.9393939393939394),\n",
              " ('question', 0.9393939393939394),\n",
              " ('court', 0.9292929292929293),\n",
              " ('United', 0.9292929292929293),\n",
              " ('grant', 0.9191919191919192),\n",
              " ('order', 0.9191919191919192),\n",
              " ('provide', 0.9191919191919192),\n",
              " ('footnote', 0.9191919191919192),\n",
              " ('Act', 0.9090909090909091),\n",
              " ('States', 0.9090909090909091),\n",
              " ('state', 0.8888888888888888)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5r_AcIMVyUa",
        "colab_type": "text"
      },
      "source": [
        "We should note that these are not tf-idf values, which are term frequencies for individual docs weighted by the inverse document frequency. This is a measure of the number of docs the words appear in weighted by inverse document frequency. We're still getting a sense of which words across the corpus and in the context of the corpus seem to have the most importance, if document frequency is a proxy for importance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx69GIqUdqtx",
        "colab_type": "text"
      },
      "source": [
        "Textacy provides access to different algorithms that can be run on docs, such as TextRank for keyword extraction. We'll start by working on a single doc, and then look at how we might scale up to thinking about the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQCRD7WTeI1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import textacy.ke"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSwQ4Rw6dWaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "9c2baae8-83c3-47f3-a1ea-d0a74f580f2a"
      },
      "source": [
        "key_terms_textrank = textacy.ke.textrank(corpus[4])\n",
        "key_terms_textrank"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('California sale tax', 0.012289366463487965),\n",
              " ('tax export', 0.011434994529752399),\n",
              " ('state tax invalid', 0.011292088146810458),\n",
              " ('retail sale tax', 0.010336929746759302),\n",
              " ('uniform sale tax', 0.009990937885509345),\n",
              " ('tax sale', 0.00972627802993417),\n",
              " ('constitutional tax immunity', 0.009330322722837766),\n",
              " ('California tax', 0.009286447078357768),\n",
              " ('federal tax o baseball bat', 0.009215411981188535),\n",
              " ('preliminary step tax free', 0.008641583345886814)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRyHfZekewCT",
        "colab_type": "text"
      },
      "source": [
        "For comparison, we'll take a look at another algorithm, Yake. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HWvJ8ube4Vc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "77f2af2c-e42d-477c-ed76-b735067646fa"
      },
      "source": [
        "key_terms_yake = textacy.ke.yake(corpus[4])\n",
        "key_terms_yake"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('California Supreme Court', 0.018873206712016664),\n",
              " ('United States', 0.046302040616416705),\n",
              " ('Export clause', 0.06387088737883487),\n",
              " ('Commerce Clause', 0.07083625701213919),\n",
              " ('tax', 0.0727455043896783),\n",
              " ('Constitution', 0.08321768623653905),\n",
              " ('state', 0.08514424816938379),\n",
              " ('export', 0.09609943886846276),\n",
              " ('Co.', 0.09702323164187558),\n",
              " ('S.Ct', 0.10770455302184588)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OarUA-BMfmV7",
        "colab_type": "text"
      },
      "source": [
        "Let's think about aggregating keywords over part of the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXNpQrZafrzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "key_terms_textrank_corpus = [textacy.ke.yake(doc) for doc in corpus[:20]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XCA9CT4f5N5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7cb7640f-6616-4c1c-c7c2-c336074ee4cd"
      },
      "source": [
        "key_terms_textrank_corpus"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('United States', 0.013160382427456057),\n",
              "  ('Mann Act', 0.05152851362859204),\n",
              "  ('Slave Traffic Act', 0.07311389826634875),\n",
              "  ('Compare United States', 0.07523382916843505),\n",
              "  ('Caminetti', 0.07928287442582554),\n",
              "  ('Congress', 0.07983070510471356),\n",
              "  ('Mr. Justice', 0.0817970829890904),\n",
              "  ('Caminetti case', 0.09653863310326538),\n",
              "  ('footnote', 0.09909130896740492),\n",
              "  ('Cir', 0.09970121374582218)],\n",
              " [('Interstate Commerce Commission', 0.02675206008244958),\n",
              "  ('Interstate Commerce Act', 0.04721703533878153),\n",
              "  ('Champlin', 0.07085573974176729),\n",
              "  ('District Court', 0.08184599378306529),\n",
              "  ('Pipe Line', 0.0872424118292021),\n",
              "  ('Champlin Refining Company', 0.08993347295335076),\n",
              "  ('Uncle Sam', 0.10441503322311402),\n",
              "  ('common carrier', 0.10675407119903242),\n",
              "  ('line', 0.11679787357568065),\n",
              "  ('pipe', 0.125798573727594)],\n",
              " [('United States', 0.005467655665997241),\n",
              "  ('indian title', 0.013957449400738007),\n",
              "  ('original indian title', 0.015809817443277835),\n",
              "  ('indian claim', 0.028868919524751922),\n",
              "  ('indian tribe', 0.03078038746216554),\n",
              "  ('indian land', 0.030845058722810563),\n",
              "  ('indian right', 0.03682999857723269),\n",
              "  ('Indians', 0.037112453448374876),\n",
              "  ('Shoshone Indians', 0.04478307175089349),\n",
              "  ('footnote', 0.04661616127876828)],\n",
              " [('Government', 0.07251141498373163),\n",
              "  ('A. Devitt Vaneck', 0.07983452860230646),\n",
              "  ('delay', 0.1511788347650493),\n",
              "  ('respondent', 0.15783496927687063),\n",
              "  ('Court', 0.159877371129936),\n",
              "  ('contract', 0.16075811930813025),\n",
              "  ('Crook', 0.17637767930599166),\n",
              "  ('Rice', 0.17637767930599166),\n",
              "  ('United States', 0.18473343529543984),\n",
              "  ('work', 0.18850525266079762)],\n",
              " [('California Supreme Court', 0.018873206712016664),\n",
              "  ('United States', 0.046302040616416705),\n",
              "  ('Export clause', 0.06387088737883487),\n",
              "  ('Commerce Clause', 0.07083625701213919),\n",
              "  ('tax', 0.0727455043896783),\n",
              "  ('Constitution', 0.08321768623653905),\n",
              "  ('state', 0.08514424816938379),\n",
              "  ('export', 0.09609943886846276),\n",
              "  ('Co.', 0.09702323164187558),\n",
              "  ('S.Ct', 0.10770455302184588)],\n",
              " [('Electric Bond', 0.017904084487482236),\n",
              "  ('Commission', 0.019371845968962083),\n",
              "  ('Trade Commission Report', 0.037540023379508965),\n",
              "  ('Share', 0.03774596508585307),\n",
              "  ('American Co.', 0.03793661607912166),\n",
              "  ('Exchange Commission', 0.03953371796835083),\n",
              "  ('Share Company', 0.041164504175063975),\n",
              "  ('North American Co.', 0.04341288099506367),\n",
              "  ('Share Co.', 0.0436751567421609),\n",
              "  ('Federal Trade Commission', 0.04452668317173932)],\n",
              " [('Circuit Court', 0.017249021272086846),\n",
              "  ('United States', 0.022768542614925384),\n",
              "  ('District Court', 0.03135372410265976),\n",
              "  ('Act', 0.05350756570348873),\n",
              "  ('Royalty Adjustment Act', 0.0564424858081666),\n",
              "  ('Alma', 0.07545808834832998),\n",
              "  ('Appeals', 0.07755658442486399),\n",
              "  ('Timken', 0.07868310697138446),\n",
              "  ('Order', 0.09462197150061526),\n",
              "  ('footnote', 0.09721425715374199)],\n",
              " [('Alaska Salmon', 0.0484464042638905),\n",
              "  ('Alaska Act', 0.05454419286906971),\n",
              "  ('Alaska Salmon Company', 0.056770905010729365),\n",
              "  ('Commission', 0.06334253775411161),\n",
              "  ('San Francisco', 0.06405087734487018),\n",
              "  ('Alaska Salmon Industry', 0.08604156501176023),\n",
              "  ('Alaska Unemployment Compensation', 0.09299835390409408),\n",
              "  ('Compensation Commission', 0.09600153410650204),\n",
              "  ('footnote', 0.09636399481104467),\n",
              "  ('Court', 0.09942651840639054)],\n",
              " [('New York', 0.02053472133059282),\n",
              "  ('New York law', 0.02990513993686249),\n",
              "  ('New York Court', 0.04280962045697454),\n",
              "  ('New York City', 0.046209974256707254),\n",
              "  ('Circuit Court', 0.0573593769422642),\n",
              "  ('interest', 0.08234277784842545),\n",
              "  ('state', 0.08754477241145554),\n",
              "  ('Bondholders Protective Committee', 0.0880176983223665),\n",
              "  ('Co.', 0.08829285972859731),\n",
              "  ('Bankruptcy Act', 0.09054482570509519)],\n",
              " [('Illinois Supreme Court', 0.015346105099784443),\n",
              "  ('United States', 0.06947102078610862),\n",
              "  ('United States Constitution', 0.07323334835808411),\n",
              "  ('Stephen A. Mitchell', 0.08566030743296747),\n",
              "  ('Illinois law', 0.10143345758617972),\n",
              "  ('York Supreme Court', 0.11020480325929967),\n",
              "  ('counsel', 0.12133684648355642),\n",
              "  ('Illinois State', 0.12280962156738434),\n",
              "  ('state', 0.12822251616416333),\n",
              "  ('Carter', 0.12914477568432678)],\n",
              " [('Circuit Court', 0.015275904961039774),\n",
              "  ('United States', 0.022784725129025834),\n",
              "  ('District Court', 0.033273749433327046),\n",
              "  ('Appeals', 0.07317837754867566),\n",
              "  ('jury', 0.08469244837955524),\n",
              "  ('Roland Rich Woolley', 0.08847657060611455),\n",
              "  ('Mr. Justice', 0.09255757652770498),\n",
              "  ('footnote', 0.1055756539546515),\n",
              "  ('court', 0.11264428527887058),\n",
              "  ('woman', 0.11331529591093545)],\n",
              " [('Bruno', 0.09627083854214705),\n",
              "  ('price', 0.1198407604271721),\n",
              "  ('Circuit Court', 0.14768412642009615),\n",
              "  ('ceiling', 0.15924134128552975),\n",
              "  ('paper', 0.15934838322369774),\n",
              "  ('Carrano', 0.16181312692333547),\n",
              "  ('ceiling price', 0.18498082788420517),\n",
              "  ('footnote', 0.21692233570797234),\n",
              "  ('Appeals', 0.23177455628503155),\n",
              "  ('customer', 0.23991832560132376)],\n",
              " [('United States', 0.025456674595820225),\n",
              "  ('Nazi', 0.10602040144251394),\n",
              "  ('Nazi party', 0.10705594970780946),\n",
              "  ('Act', 0.12651210998640022),\n",
              "  ('Draeger', 0.12696993422706768),\n",
              "  ('footnote', 0.13750780193164175),\n",
              "  ('Fiswick', 0.1387144276059529),\n",
              "  ('Vogel', 0.14111556649644694),\n",
              "  ('Court', 0.14268703011487402),\n",
              "  ('conspiracy', 0.14800794723159183)],\n",
              " [('Federal Communications Commission', 0.016332360102184834),\n",
              "  ('Federal Radio Commission', 0.03105496600160018),\n",
              "  ('Communications Act', 0.14622217315660055),\n",
              "  ('station', 0.16090996764121507),\n",
              "  ('case', 0.18768082226967228),\n",
              "  ('fact', 0.18774820041272766),\n",
              "  ('Court', 0.19189086918502246),\n",
              "  ('Columbia', 0.1932204048925655),\n",
              "  ('license', 0.21049980097907273),\n",
              "  ('U.S.C.A.', 0.21346950913333945)],\n",
              " [('United States', 0.01120361815946902),\n",
              "  ('Federal Works Administrator', 0.02305878992032556),\n",
              "  ('Public Buildings Act', 0.031073920812789514),\n",
              "  ('Federal Government', 0.03290441148924627),\n",
              "  ('United States District', 0.0359034112780017),\n",
              "  ('Federal Works Agency', 0.04312582393579285),\n",
              "  ('United States post', 0.04323264684124367),\n",
              "  ('United States government', 0.04403517510321035),\n",
              "  ('States District Court', 0.05529857275437153),\n",
              "  ('Cape Girardeau', 0.07021878774123161)],\n",
              " [('States Express Co.', 0.029319188321963025),\n",
              "  ('Adams Mfg', 0.03197415811721528),\n",
              "  ('White Co.', 0.03357729218061959),\n",
              "  ('state', 0.03944709766080884),\n",
              "  ('Adams case', 0.04216341178530251),\n",
              "  ('Mining Co.', 0.04505811599818764),\n",
              "  ('Coal Mining Co.', 0.0467514272505261),\n",
              "  ('Indiana', 0.046768629179091095),\n",
              "  ('footnote', 0.04767687272859613),\n",
              "  ('Indiana tax', 0.04920747937124936)],\n",
              " [('Secretary', 0.05886185818419686),\n",
              "  ('United States', 0.061949906815344694),\n",
              "  ('Marketing Agreement Act', 0.07435073563920688),\n",
              "  ('Agriculture', 0.09233464065047925),\n",
              "  ('Congress', 0.10083519311168773),\n",
              "  ('Fund', 0.11737696784996321),\n",
              "  ('handler', 0.1271115022460891),\n",
              "  ('Agricultural Marketing Agreement', 0.13217085812386825),\n",
              "  ('order', 0.13506113978723389),\n",
              "  ('court', 0.13635104174929644)],\n",
              " [('Government', 0.11670835633743407),\n",
              "  ('Court', 0.11707217707223798),\n",
              "  ('tax', 0.13712243848244476),\n",
              "  ('claim', 0.14489945708523186),\n",
              "  ('refund', 0.15209182751412398),\n",
              "  ('recoupment', 0.16121570594703458),\n",
              "  ('Co.', 0.16140770295148568),\n",
              "  ('statute', 0.17346886363362435),\n",
              "  ('Bull', 0.17472600518290207),\n",
              "  ('Storage Battery Co.', 0.18156875963946265)],\n",
              " [('United States', 0.04955472256331873),\n",
              "  ('Samuels', 0.0769814990145342),\n",
              "  ('Fed', 0.09956836929497823),\n",
              "  ('States ex', 0.10622158124372695),\n",
              "  ('board', 0.10785148175759833),\n",
              "  ('Act', 0.10810181225782986),\n",
              "  ('v.', 0.12320146782991971),\n",
              "  ('panel', 0.12673781003498932),\n",
              "  ('District Court', 0.1301598932239707),\n",
              "  ('Selective Service', 0.14531097214948474)],\n",
              " [('Horowitz', 0.10860942153328497),\n",
              "  ('board', 0.12391531357790884),\n",
              "  ('Samuels', 0.13218391082391076),\n",
              "  ('panel', 0.15572950500264393),\n",
              "  ('local', 0.18961489731451703),\n",
              "  ('local board', 0.19407373234701047),\n",
              "  ('Court', 0.19409390489289857),\n",
              "  ('classification', 0.19609096232141635),\n",
              "  ('case', 0.1999733212499275),\n",
              "  ('District Court', 0.20439222263733844)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4EJrV3EgBcs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a89d72b-b97f-43ae-c69b-5e71c254d2ef"
      },
      "source": [
        "flat_list = [item for sublist in key_terms_textrank_corpus for item in sublist]\n",
        "flat_list"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('United States', 0.013160382427456057),\n",
              " ('Mann Act', 0.05152851362859204),\n",
              " ('Slave Traffic Act', 0.07311389826634875),\n",
              " ('Compare United States', 0.07523382916843505),\n",
              " ('Caminetti', 0.07928287442582554),\n",
              " ('Congress', 0.07983070510471356),\n",
              " ('Mr. Justice', 0.0817970829890904),\n",
              " ('Caminetti case', 0.09653863310326538),\n",
              " ('footnote', 0.09909130896740492),\n",
              " ('Cir', 0.09970121374582218),\n",
              " ('Interstate Commerce Commission', 0.02675206008244958),\n",
              " ('Interstate Commerce Act', 0.04721703533878153),\n",
              " ('Champlin', 0.07085573974176729),\n",
              " ('District Court', 0.08184599378306529),\n",
              " ('Pipe Line', 0.0872424118292021),\n",
              " ('Champlin Refining Company', 0.08993347295335076),\n",
              " ('Uncle Sam', 0.10441503322311402),\n",
              " ('common carrier', 0.10675407119903242),\n",
              " ('line', 0.11679787357568065),\n",
              " ('pipe', 0.125798573727594),\n",
              " ('United States', 0.005467655665997241),\n",
              " ('indian title', 0.013957449400738007),\n",
              " ('original indian title', 0.015809817443277835),\n",
              " ('indian claim', 0.028868919524751922),\n",
              " ('indian tribe', 0.03078038746216554),\n",
              " ('indian land', 0.030845058722810563),\n",
              " ('indian right', 0.03682999857723269),\n",
              " ('Indians', 0.037112453448374876),\n",
              " ('Shoshone Indians', 0.04478307175089349),\n",
              " ('footnote', 0.04661616127876828),\n",
              " ('Government', 0.07251141498373163),\n",
              " ('A. Devitt Vaneck', 0.07983452860230646),\n",
              " ('delay', 0.1511788347650493),\n",
              " ('respondent', 0.15783496927687063),\n",
              " ('Court', 0.159877371129936),\n",
              " ('contract', 0.16075811930813025),\n",
              " ('Crook', 0.17637767930599166),\n",
              " ('Rice', 0.17637767930599166),\n",
              " ('United States', 0.18473343529543984),\n",
              " ('work', 0.18850525266079762),\n",
              " ('California Supreme Court', 0.018873206712016664),\n",
              " ('United States', 0.046302040616416705),\n",
              " ('Export clause', 0.06387088737883487),\n",
              " ('Commerce Clause', 0.07083625701213919),\n",
              " ('tax', 0.0727455043896783),\n",
              " ('Constitution', 0.08321768623653905),\n",
              " ('state', 0.08514424816938379),\n",
              " ('export', 0.09609943886846276),\n",
              " ('Co.', 0.09702323164187558),\n",
              " ('S.Ct', 0.10770455302184588),\n",
              " ('Electric Bond', 0.017904084487482236),\n",
              " ('Commission', 0.019371845968962083),\n",
              " ('Trade Commission Report', 0.037540023379508965),\n",
              " ('Share', 0.03774596508585307),\n",
              " ('American Co.', 0.03793661607912166),\n",
              " ('Exchange Commission', 0.03953371796835083),\n",
              " ('Share Company', 0.041164504175063975),\n",
              " ('North American Co.', 0.04341288099506367),\n",
              " ('Share Co.', 0.0436751567421609),\n",
              " ('Federal Trade Commission', 0.04452668317173932),\n",
              " ('Circuit Court', 0.017249021272086846),\n",
              " ('United States', 0.022768542614925384),\n",
              " ('District Court', 0.03135372410265976),\n",
              " ('Act', 0.05350756570348873),\n",
              " ('Royalty Adjustment Act', 0.0564424858081666),\n",
              " ('Alma', 0.07545808834832998),\n",
              " ('Appeals', 0.07755658442486399),\n",
              " ('Timken', 0.07868310697138446),\n",
              " ('Order', 0.09462197150061526),\n",
              " ('footnote', 0.09721425715374199),\n",
              " ('Alaska Salmon', 0.0484464042638905),\n",
              " ('Alaska Act', 0.05454419286906971),\n",
              " ('Alaska Salmon Company', 0.056770905010729365),\n",
              " ('Commission', 0.06334253775411161),\n",
              " ('San Francisco', 0.06405087734487018),\n",
              " ('Alaska Salmon Industry', 0.08604156501176023),\n",
              " ('Alaska Unemployment Compensation', 0.09299835390409408),\n",
              " ('Compensation Commission', 0.09600153410650204),\n",
              " ('footnote', 0.09636399481104467),\n",
              " ('Court', 0.09942651840639054),\n",
              " ('New York', 0.02053472133059282),\n",
              " ('New York law', 0.02990513993686249),\n",
              " ('New York Court', 0.04280962045697454),\n",
              " ('New York City', 0.046209974256707254),\n",
              " ('Circuit Court', 0.0573593769422642),\n",
              " ('interest', 0.08234277784842545),\n",
              " ('state', 0.08754477241145554),\n",
              " ('Bondholders Protective Committee', 0.0880176983223665),\n",
              " ('Co.', 0.08829285972859731),\n",
              " ('Bankruptcy Act', 0.09054482570509519),\n",
              " ('Illinois Supreme Court', 0.015346105099784443),\n",
              " ('United States', 0.06947102078610862),\n",
              " ('United States Constitution', 0.07323334835808411),\n",
              " ('Stephen A. Mitchell', 0.08566030743296747),\n",
              " ('Illinois law', 0.10143345758617972),\n",
              " ('York Supreme Court', 0.11020480325929967),\n",
              " ('counsel', 0.12133684648355642),\n",
              " ('Illinois State', 0.12280962156738434),\n",
              " ('state', 0.12822251616416333),\n",
              " ('Carter', 0.12914477568432678),\n",
              " ('Circuit Court', 0.015275904961039774),\n",
              " ('United States', 0.022784725129025834),\n",
              " ('District Court', 0.033273749433327046),\n",
              " ('Appeals', 0.07317837754867566),\n",
              " ('jury', 0.08469244837955524),\n",
              " ('Roland Rich Woolley', 0.08847657060611455),\n",
              " ('Mr. Justice', 0.09255757652770498),\n",
              " ('footnote', 0.1055756539546515),\n",
              " ('court', 0.11264428527887058),\n",
              " ('woman', 0.11331529591093545),\n",
              " ('Bruno', 0.09627083854214705),\n",
              " ('price', 0.1198407604271721),\n",
              " ('Circuit Court', 0.14768412642009615),\n",
              " ('ceiling', 0.15924134128552975),\n",
              " ('paper', 0.15934838322369774),\n",
              " ('Carrano', 0.16181312692333547),\n",
              " ('ceiling price', 0.18498082788420517),\n",
              " ('footnote', 0.21692233570797234),\n",
              " ('Appeals', 0.23177455628503155),\n",
              " ('customer', 0.23991832560132376),\n",
              " ('United States', 0.025456674595820225),\n",
              " ('Nazi', 0.10602040144251394),\n",
              " ('Nazi party', 0.10705594970780946),\n",
              " ('Act', 0.12651210998640022),\n",
              " ('Draeger', 0.12696993422706768),\n",
              " ('footnote', 0.13750780193164175),\n",
              " ('Fiswick', 0.1387144276059529),\n",
              " ('Vogel', 0.14111556649644694),\n",
              " ('Court', 0.14268703011487402),\n",
              " ('conspiracy', 0.14800794723159183),\n",
              " ('Federal Communications Commission', 0.016332360102184834),\n",
              " ('Federal Radio Commission', 0.03105496600160018),\n",
              " ('Communications Act', 0.14622217315660055),\n",
              " ('station', 0.16090996764121507),\n",
              " ('case', 0.18768082226967228),\n",
              " ('fact', 0.18774820041272766),\n",
              " ('Court', 0.19189086918502246),\n",
              " ('Columbia', 0.1932204048925655),\n",
              " ('license', 0.21049980097907273),\n",
              " ('U.S.C.A.', 0.21346950913333945),\n",
              " ('United States', 0.01120361815946902),\n",
              " ('Federal Works Administrator', 0.02305878992032556),\n",
              " ('Public Buildings Act', 0.031073920812789514),\n",
              " ('Federal Government', 0.03290441148924627),\n",
              " ('United States District', 0.0359034112780017),\n",
              " ('Federal Works Agency', 0.04312582393579285),\n",
              " ('United States post', 0.04323264684124367),\n",
              " ('United States government', 0.04403517510321035),\n",
              " ('States District Court', 0.05529857275437153),\n",
              " ('Cape Girardeau', 0.07021878774123161),\n",
              " ('States Express Co.', 0.029319188321963025),\n",
              " ('Adams Mfg', 0.03197415811721528),\n",
              " ('White Co.', 0.03357729218061959),\n",
              " ('state', 0.03944709766080884),\n",
              " ('Adams case', 0.04216341178530251),\n",
              " ('Mining Co.', 0.04505811599818764),\n",
              " ('Coal Mining Co.', 0.0467514272505261),\n",
              " ('Indiana', 0.046768629179091095),\n",
              " ('footnote', 0.04767687272859613),\n",
              " ('Indiana tax', 0.04920747937124936),\n",
              " ('Secretary', 0.05886185818419686),\n",
              " ('United States', 0.061949906815344694),\n",
              " ('Marketing Agreement Act', 0.07435073563920688),\n",
              " ('Agriculture', 0.09233464065047925),\n",
              " ('Congress', 0.10083519311168773),\n",
              " ('Fund', 0.11737696784996321),\n",
              " ('handler', 0.1271115022460891),\n",
              " ('Agricultural Marketing Agreement', 0.13217085812386825),\n",
              " ('order', 0.13506113978723389),\n",
              " ('court', 0.13635104174929644),\n",
              " ('Government', 0.11670835633743407),\n",
              " ('Court', 0.11707217707223798),\n",
              " ('tax', 0.13712243848244476),\n",
              " ('claim', 0.14489945708523186),\n",
              " ('refund', 0.15209182751412398),\n",
              " ('recoupment', 0.16121570594703458),\n",
              " ('Co.', 0.16140770295148568),\n",
              " ('statute', 0.17346886363362435),\n",
              " ('Bull', 0.17472600518290207),\n",
              " ('Storage Battery Co.', 0.18156875963946265),\n",
              " ('United States', 0.04955472256331873),\n",
              " ('Samuels', 0.0769814990145342),\n",
              " ('Fed', 0.09956836929497823),\n",
              " ('States ex', 0.10622158124372695),\n",
              " ('board', 0.10785148175759833),\n",
              " ('Act', 0.10810181225782986),\n",
              " ('v.', 0.12320146782991971),\n",
              " ('panel', 0.12673781003498932),\n",
              " ('District Court', 0.1301598932239707),\n",
              " ('Selective Service', 0.14531097214948474),\n",
              " ('Horowitz', 0.10860942153328497),\n",
              " ('board', 0.12391531357790884),\n",
              " ('Samuels', 0.13218391082391076),\n",
              " ('panel', 0.15572950500264393),\n",
              " ('local', 0.18961489731451703),\n",
              " ('local board', 0.19407373234701047),\n",
              " ('Court', 0.19409390489289857),\n",
              " ('classification', 0.19609096232141635),\n",
              " ('case', 0.1999733212499275),\n",
              " ('District Court', 0.20439222263733844)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3HHZLIXgawc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "d39fbb74-5e9c-47d8-b485-8c2276666acd"
      },
      "source": [
        "keyword_counter = Counter(flat_list)\n",
        "keyword_counter.most_common(20)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('United States', 11),\n",
              " ('footnote', 8),\n",
              " ('Court', 6),\n",
              " ('District Court', 5),\n",
              " ('state', 4),\n",
              " ('Circuit Court', 4),\n",
              " ('Co.', 3),\n",
              " ('Act', 3),\n",
              " ('Appeals', 3),\n",
              " ('Congress', 2),\n",
              " ('Mr. Justice', 2),\n",
              " ('Government', 2),\n",
              " ('tax', 2),\n",
              " ('Commission', 2),\n",
              " ('court', 2),\n",
              " ('case', 2),\n",
              " ('Samuels', 2),\n",
              " ('board', 2),\n",
              " ('panel', 2),\n",
              " ('Mann Act', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P296E9nxhnQZ",
        "colab_type": "text"
      },
      "source": [
        "### Activity:\n",
        "Let's combine a few different pieces. Try filtering the corpus on some metadata to construct a sub-corpus. Then use one of the textacy keyword algorithms to determine the most common keywords across your subcorpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_ZuOB3_h4Wn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuSHX_2OVyUb",
        "colab_type": "text"
      },
      "source": [
        "## Vectorization\n",
        "\n",
        "Let's continue with corpus level analysis by taking advantage of textacy's vectorizer class, which wraps functionality from scikit-learn. We could just work directly in scikit-learn, but it can be nice for mental overhead to learn one library and be able to do a great deal with it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pFXy9VK7DXa",
        "colab_type": "text"
      },
      "source": [
        "We'll create a vectorizer, sticking with the normal term frequency defaults but discarding words that appear in less than 3 documents or more than 95% of documents. We'll also limit our features to the top 500 words according to document frequency.This means our feature set, or columns, will have a higher degree of representation across the corpus. We could vectorize according to tf-idf as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LfE48GbVyUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "58a64fa6-ba89-44c0-e1ed-65eba1a521aa"
      },
      "source": [
        "import textacy.vsm\n",
        "\n",
        "vectorizer = textacy.vsm.Vectorizer(min_df=3, max_df=.95, max_n_terms=500)\n",
        "tokenized_corpus = (doc._.to_terms_list(ngrams=1, as_strings=True,\n",
        "                                        filter_punct=True, \n",
        "                                        filter_stops=True, \n",
        "                                        filter_nums=True \n",
        "                                        ) for doc in corpus)\n",
        "dtm = vectorizer.fit_transform(tokenized_corpus)\n",
        "dtm"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<99x500 sparse matrix of type '<class 'numpy.int32'>'\n",
              "\twith 24902 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5lQ10twVyUf",
        "colab_type": "text"
      },
      "source": [
        "We have now have a matrix representation of our corpus, where rows are documents, and columns (or features) are words from the corpus. The value at any given point is the number of times that the word appears in that document. Once we have a document-term matrix, we could do a few different things with it, just within textacy, though we could take it and pass it into different algorithms within scikit-learn or other libraries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37KLE01TVyUg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "9f56699e-f41d-4832-ab40-268c46ee92f4"
      },
      "source": [
        "# Let's first look at some of the terms\n",
        "vectorizer.terms_list[:20]"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['$',\n",
              " '1',\n",
              " '10',\n",
              " '11',\n",
              " '12',\n",
              " '13',\n",
              " '14',\n",
              " '15',\n",
              " '16',\n",
              " '18',\n",
              " '2',\n",
              " '28',\n",
              " '29',\n",
              " '3',\n",
              " '4',\n",
              " '49',\n",
              " '5',\n",
              " '50',\n",
              " '6',\n",
              " '7']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_I6y0JIVyUh",
        "colab_type": "text"
      },
      "source": [
        "We can see that we are still getting a number of terms which ought to be filtered out, such as numbers and punctuation. We would want to clean this up more before vectorizing in the future. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ0p4VLvVyUi",
        "colab_type": "text"
      },
      "source": [
        "## Further Topics\n",
        "\n",
        "Things to do:\n",
        "\n",
        "- textacy: extract ngrams\n",
        "- textacy: pagerank and sgrank\n",
        "- word co-occurence matrices?\n",
        "- custom cleaning?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqKyBkCnVyUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGPax010VyVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}