{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "206px",
        "width": "555px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "Text Analysis with Python.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGRR3FVUVyQb",
        "colab_type": "text"
      },
      "source": [
        "<center>\n",
        "  <h1>Digital Tools and Methods for the Humanities and Social Sciences</h1>\n",
        "  <img src=\"https://raw.githubusercontent.com/sul-cidr/Workshops/master/cidr-logo.no-text.240x140.png\" alt=\"Center for Interdisciplinary Digital Research @ Stanford\"/>\n",
        "</center>\n",
        "\n",
        "<h1>Text Analysis with Python</h1>\n",
        "\n",
        "## Front-Matter\n",
        "\n",
        "### Instructors\n",
        "- Scott Bailey (CIDR), <em>scottbailey@stanford.edu</em>\n",
        "- Simon Wiles (CIDR), <em>simon.wiles@stanford.edu</em>\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "Develop practical knowledge of an end-to-end workflow for text analysis in Python using two specific libraries: spaCy and textacy.\n",
        "\n",
        "- Import data\n",
        "- Clean/preprocess text data\n",
        "- Analyze single documents\n",
        "- Analyze a full corpus\n",
        "\n",
        "\n",
        "### Topics\n",
        "\n",
        "- Document Tokenization\n",
        "- Part-of-Speech (POS) Tagging\n",
        "- Named-Entity Recognition (NER)\n",
        "- Corpus Analysis and Vectorization\n",
        "\n",
        "\n",
        "### Jupyter Notebooks and Google Colaboratory\n",
        "\n",
        "Jupyter notebooks are a way to write and run Python code in an interactive way. They're quickly becoming a standard way of putting together data, code, and written explanations or visualizations into a single document and sharing that. There are a lot of ways that you can run Jupyter notebooks, including just locally on your computer, but we've decided to use Google's Colaboratory notebook platform for this workshop.  Colaboratory is “a Google research project created to help disseminate machine learning education and research.”  If you would like to know more about Colaboratory in general, you can visit the [Welcome Notebook](https://colab.research.google.com/notebooks/welcome.ipynb).\n",
        "\n",
        "Using the Google Colaboratory platform allows us to focus on learning and writing Python in the workshop rather than on setting up Python, which can sometimes take a bit of extra work depending on platforms, operating systems, and other installed applications. If you'd like to install a Python distribution locally, though, we have some instructions (with gifs!) on installing Python through the Anaconda distribution, which will also help you handle virtual environments: https://github.com/sul-cidr/Workshops/wiki/Installing-and-Configuring-Anaconda-and-Jupyter-Notebooks\n",
        "\n",
        "If you run into problems, or would like to look into other ways of installing Python or handling virtual environments, feel free to send us an email (contact-cidr@stanford.edu) or visit us during our [consulting hours](https://library.stanford.edu/research/cidr/consulting).\n",
        "\n",
        "### Environment\n",
        "If you would prefer to use Anaconda or your own local installation of python or Jupyter Notebooks, for this workshop you will need an environment with the following packages installed and available:\n",
        "- `spacy`\n",
        "- `textacy`\n",
        "\n",
        "Please note that we will not have time during the workshop to support you with problems related to a local environment, and we do recommend using the Colaboratory notebooks if you are at all unsure.\n",
        "\n",
        "### Evaluation survey\n",
        "At the end of the workshop, we would be very grateful if you can, please, spend 1 minute answering a few questions that will help us to continue our workshop series.\n",
        "- https://stanforduniversity.qualtrics.com/jfe/form/SV_cIqXu1piN6JTeJv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEFaMNm3VyQf",
        "colab_type": "text"
      },
      "source": [
        "# Document-level Analysis with `spaCy`\n",
        "\n",
        "Let's start by learning how spaCy works, and using it to start analyzing a single textual document. We'll work with some sample data throughout, but talk through importing larger corpora later in the workshop. \n",
        "\n",
        "For now, we'll start with imports, setting up the model, and working with a short text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1u3OoHQVyQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRVkmEOvVyQq",
        "colab_type": "text"
      },
      "source": [
        "spaCy uses pre-trained neural network models to process text. Here we're going to download and use a medium-sized English multi-task CNN, which has high accuracy for part of speech tagging, entity recognition, and includes word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3lrUP1cVyQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31NWduWIVyQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Once we've installed the model, we can load it like any other Python library\n",
        "import en_core_web_md"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT2rin_fVyQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the language model\n",
        "nlp = en_core_web_md.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQYz476fVyRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From H.G. Well's A Short History of the World, Project Gutenberg \n",
        "text = \"\"\"Even under the Assyrian monarchs and especially under\n",
        "Sardanapalus, Babylon had been a scene of great intellectual\n",
        "activity.  {111} Sardanapalus, though an Assyrian, had been quite\n",
        "Babylon-ized.  He made a library, a library not of paper but of\n",
        "the clay tablets that were used for writing in Mesopotamia since\n",
        "early Sumerian days.  His collection has been unearthed and is\n",
        "perhaps the most precious store of historical material in the\n",
        "world.  The last of the Chaldean line of Babylonian monarchs,\n",
        "Nabonidus, had even keener literary tastes.  He patronized\n",
        "antiquarian researches, and when a date was worked out by his\n",
        "investigators for the accession of Sargon I he commemorated the\n",
        "fact by inscriptions.  But there were many signs of disunion in\n",
        "his empire, and he sought to centralize it by bringing a number of\n",
        "the various local gods to Babylon and setting up temples to them\n",
        "there.  This device was to be practised quite successfully by the\n",
        "Romans in later times, but in Babylon it roused the jealousy of\n",
        "the powerful priesthood of Bel Marduk, the dominant god of the\n",
        "Babylonians.  They cast about for a possible alternative to\n",
        "Nabonidus and found it in Cyrus the Persian, the ruler of the\n",
        "adjacent Median Empire.  Cyrus had already distinguished himself\n",
        "by conquering Croesus, the rich king of Lydia in Eastern Asia\n",
        "Minor.  {112} He came up against Babylon, there was a battle\n",
        "outside the walls, and the gates of the city were opened to him\n",
        "(538 B.C.).  His soldiers entered the city without fighting.  The\n",
        "crown prince Belshazzar, the son of Nabonidus, was feasting, the\n",
        "Bible relates, when a hand appeared and wrote in letters of fire\n",
        "upon the wall these mystical words: _\"Mene, Mene, Tekel,\n",
        "Upharsin,\"_ which was interpreted by the prophet Daniel, whom he\n",
        "summoned to read the riddle, as \"God has numbered thy kingdom and\n",
        "finished it; thou art weighed in the balance and found wanting and\n",
        "thy kingdom is given to the Medes and Persians.\"  Possibly the\n",
        "priests of Bel Marduk knew something about that writing on the\n",
        "wall.  Belshazzar was killed that night, says the Bible.\n",
        "Nabonidus was taken prisoner, and the occupation of the city was\n",
        "so peaceful that the services of Bel Marduk continued without\n",
        "intermission.\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR5v_iE3VyRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9pYgwcqVyRL",
        "colab_type": "text"
      },
      "source": [
        "Once we pass the text into the NLP model, spaCy processes the entire text and makes many features available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnkTWvuwVyRN",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "The doc created by spaCy immediately provides access to the word level tokens of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg6EB7WeVyRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for token in doc[:15]:\n",
        "  print(token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh2z0VkgVyRW",
        "colab_type": "text"
      },
      "source": [
        "Each of these tokens has a number of properties, and we'll look a bit more closely at this in a minute when we think about preprocessing texts, but let's continue our quick tour. \n",
        "\n",
        "spaCy also automatically provides sentence level tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Rr6LvXVyRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sent in doc.sents:\n",
        "    print(sent.text + \"\\n--\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcwG9tH9VyRd",
        "colab_type": "text"
      },
      "source": [
        "We can collect both words and sentences into standard Python data structures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwnDSjL7VyRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [sent.text for sent in doc.sents]\n",
        "sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVJF1L5PVyRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = [token.text for token in doc]\n",
        "words[:30]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xndApEFuVyRn",
        "colab_type": "text"
      },
      "source": [
        "### Filtering Tokens\n",
        "\n",
        "Let's start with cleaning the text and counting to see what we can learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSHaSQWqVyRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One of the common things we do in text analysis is to remove punctuation\n",
        "no_punct = [token for token in doc if token.is_punct == False]\n",
        "for token in no_punct[:20]:\n",
        "  print(token.text, token.is_punct)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K63rP_PJVyRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This has worked, but left in new line characters and spaces\n",
        "no_punct_or_space = [token for token in doc if token.is_punct == False and token.is_space == False]\n",
        "for token in no_punct_or_space[:30]:\n",
        "  print(token.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHjzgbbgVyRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's say we also want to remove numbers, and lowercase everything\n",
        "lower_alpha = [token.lower_ for token in no_punct_or_space if token.is_alpha == True]\n",
        "lower_alpha[:30]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbpLhAqnVyR1",
        "colab_type": "text"
      },
      "source": [
        "One other common bit of preprocessing is to remove stopwords, that is, the common words in a language that don't convey the information that we are looking for in our analysis. For example, if we looked for the most common words in a text, we would want to remove stopwords so that we don't only get words such as 'a,' 'the,' and 'and.'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STyEpj96VyR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean = [token.lower_ for token in no_punct_or_space if token.is_alpha == True and token.is_stop == False]\n",
        "clean[:30]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWe736X7mwKF",
        "colab_type": "text"
      },
      "source": [
        "For this piece, we've used spaCy's built in stopword list, which is used to create the property `is_stop` for each token. There's a good chance you would want to create custom stopwords lists though, especially if you're working with historical text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP8b8upcmx5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We'll just pick a couple of words we know are in the example\n",
        "custom_stopwords = [\"assyrian\", \"babylon\"]\n",
        "\n",
        "custom_clean = [token.lower_ for token in doc if token.lower_ not in custom_stopwords]\n",
        "custom_clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ENXxjBHVyR8",
        "colab_type": "text"
      },
      "source": [
        "At this point, we have a list of lower-cased tokens that doesn't contain punctuation, white-space, numbers, or stopwords. Depending on our analysis, we may or may not want to do this much cleaning. But, it is good to understand how much we can do just with spaCy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSGJfxiaVyR-",
        "colab_type": "text"
      },
      "source": [
        "### Counting Tokens\n",
        "\n",
        "Let's then look at what we can do now that we have groups of tokens at different lengths. We can start with just counting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEFjnPPLVySA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Number of tokens in document: \", len(doc))\n",
        "print(\"Number of tokens in cleaned document: \", len(clean))\n",
        "print(\"Number of unique tokens in cleaned document: \", len(set(clean)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6JRDl22VySL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZwZ3En1VySY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_counter = Counter([token.lower_ for token in doc])\n",
        "full_counter.most_common(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIrMQFp6VySg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_counter = Counter(clean)\n",
        "cleaned_counter.most_common(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL9FAgJjVySu",
        "colab_type": "text"
      },
      "source": [
        "**Question:** Why do we have to use a list comprehension for the non-clean doc while we can just pass a variable directly for the cleaned set of tokens?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRNYHP7wVySv",
        "colab_type": "text"
      },
      "source": [
        "## Part-of-Speech Tagging\n",
        "\n",
        "Let's turn to the other aspects of the text that spaCy exposes for us. Depending on what questions we might have about the text, these will be more or less helpful. \n",
        "\n",
        "We'll start with parts of speech. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLVUUOT9VySw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# spaCy provides two levels of POS tagging. Here's the more general.\n",
        "for token in doc[:30]:\n",
        "  print(token.text, token.pos_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbB2eeMTVyS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We also have the more specific Penn Treenbank tags.\n",
        "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
        "for token in doc[:30]:\n",
        "  print(token.text, token.tag_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6X3cbcmVyS4",
        "colab_type": "text"
      },
      "source": [
        "We can accumulate the groups of tokens by way of these in order understand distributions of parts of speech throughout the text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVey8EXsVyS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nouns = [token for token in doc if token.pos_ == \"NOUN\"]\n",
        "verbs = [token for token in doc if token.pos_ == \"VERB\"]\n",
        "proper_nouns = [token for token in doc if token.pos_ == \"PROPN\"]\n",
        "adjectives = [token for token in doc if token.pos_ == \"ADJ\"]\n",
        "adverbs = [token for token in doc if token.pos_ == \"ADV\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp6pH7VjVyTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_counts = {\n",
        "    \"nouns\": len(nouns),\n",
        "    \"verbs\": len(verbs),\n",
        "    \"proper_nouns\": len(proper_nouns),\n",
        "    \"adjectives\": len(adjectives),\n",
        "    \"adverbs\": len(adverbs) \n",
        "}\n",
        "\n",
        "pos_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_1y3C5LVyTP",
        "colab_type": "text"
      },
      "source": [
        "spaCy also provides full dependency parsing, but we're going to leave that alone for the moment. We'll turn instead to named entity recognition. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29Mqf_S0VyTR",
        "colab_type": "text"
      },
      "source": [
        "## Named-Entity Recognition\n",
        "\n",
        "https://spacy.io/api/annotation#named-entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KodfOLmHVyTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGCSdUMAVyTa",
        "colab_type": "text"
      },
      "source": [
        "What if we only care about geo-political entities or locations?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhIk-M0DVyTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ent_filtered = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in [\"GPE\", \"LOC\"]]\n",
        "ent_filtered"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buJBVUPQVyTe",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing Parses\n",
        "\n",
        "spaCy also has a nice built-in visualizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3Ra-HtPVyTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy import displacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIO_FEoLVyTi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU5dIAnMiODg",
        "colab_type": "text"
      },
      "source": [
        "### Activity\n",
        "\n",
        "Pick either a particular part of speech or a named entity type, and write code to determine the most common words of that type. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mro3MhI-ieQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F5P7cbMVyTl",
        "colab_type": "text"
      },
      "source": [
        "# Corpus-level Analysis with `textacy`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCwkgf9pVyTl",
        "colab_type": "text"
      },
      "source": [
        "Let's shift to thinking about a whole corpus rather than a single document.\n",
        "\n",
        "In doing so, we could keep working with spaCy directly if the features that it exposes help us answer the research questions we are asking. \n",
        "\n",
        "Instead, though, we're going to take advantage of textacy, a library built on spaCy that adds features, including a sense of a Corpus and built in analytics on it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp3AcJezVyTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install textacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMqTW64-VyTp",
        "colab_type": "text"
      },
      "source": [
        "## Generating Corpora\n",
        "\n",
        "We'll use some of the data that is included in textacy as our corpus. You could absolutely import data otherwise, whether through reading in plain text or xml files, or pulling text data and metadata from a csv file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8HmDN36VyTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import textacy\n",
        "import textacy.datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYKe7-BCVyTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We'll work with some Supreme Court cases: https://chartbeat-labs.github.io/textacy/_modules/textacy/datasets/supreme_court.html\n",
        "data = textacy.datasets.SupremeCourt()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xmidNbxVyTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.download()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9X-twUBVyTw",
        "colab_type": "text"
      },
      "source": [
        "What we have here is a collection of Supreme Court decisions, both full text and metadata. \n",
        "\n",
        "Let's look at a single one to see what we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHMbaig9VyTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "single = list(data.texts(limit=1))[0]\n",
        "single[:200]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4Xf3CkM9dJS",
        "colab_type": "text"
      },
      "source": [
        "Let's go ahead and pull a full set of texts with metadata. To keep it a bit more manageable time-wise, we'll only collect 100 of the records."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZk5b7zxVyTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "records = data.records(limit=100)\n",
        "\n",
        "# Records here is a generator - we can look at the first record by passing it to the next function.\n",
        "next(records)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TwurMgHVyT0",
        "colab_type": "text"
      },
      "source": [
        "textacy includes the idea of a corpus, while spaCy only has an idea of a single documents, though you can compose documents in standard Python data structures. Every corpus takes some texts or text plus metadata, along with a language model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gdHIQAyVyT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = textacy.Corpus(nlp, data=records)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_0YOC-kVyT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBnvE5ZJVyT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[doc._.preview for doc in corpus[:5]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuBjFKQ4VyT8",
        "colab_type": "text"
      },
      "source": [
        "We can see that the type of each item in the corpus is a `Doc` - this is effectively a spaCy doc with all of the calculated features. Textacy does give you some capacity to work with those features through it's API, and also exposes new features, such as ngrams and ranking algorithms for single documents. We'll come back to these once we work a bit at the corpus level. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DFWqESvVyT8",
        "colab_type": "text"
      },
      "source": [
        "We can filter this corpus based on metadata once we make it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-CqNgQaVyUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we'll find all the cases where the number of justices voting in the majority was greater than 6. \n",
        "recent = [doc for doc in corpus.get(lambda doc: doc._.meta[\"n_maj_votes\"] > 6)]\n",
        "len(recent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o5hy0pCVyUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "recent[0]._.preview"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8YO2bgIVyUM",
        "colab_type": "text"
      },
      "source": [
        "## Analyzing the Corpus\n",
        "\n",
        "Let's look at what we get out of the box from textacy once we've built a corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wOMpBE0VyUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"number of documents: \", corpus.n_docs)\n",
        "print(\"number of sentences: \", corpus.n_sents)\n",
        "print(\"number of tokens: \", corpus.n_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiQQrx5SVyUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We'll pass as_strings so that the results we look at will give us strings rather than unique ids.\n",
        "counts = corpus.word_counts(as_strings=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWwr9r1fFOuE",
        "colab_type": "text"
      },
      "source": [
        "Notice that, by default, the `word_counts` function is doing a certain amount of cleaning for you: https://chartbeat-labs.github.io/textacy/api_reference/lang_doc_corpus.html#textacy.corpus.Corpus.word_counts "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blFXigFMVyUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sorted(counts.items(), key=lambda x: x[1], reverse=True)[:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYx5VX51H8SF",
        "colab_type": "text"
      },
      "source": [
        "For an explanation of `-PRON-`, see https://spacy.io/api/annotation#lemmatization. Basically it's spaCy's way of lemmatizing pronouns. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELd2rxUZVyUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_doc_counts = corpus.word_doc_counts(weighting=\"freq\", smooth_idf=True, filter_stops=True, as_strings=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIIkImS2VyUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sorted(word_doc_counts.items(), key=lambda x:x[1], reverse=True)[:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5r_AcIMVyUa",
        "colab_type": "text"
      },
      "source": [
        "We should note that these are not tf-idf values, which are term frequencies for individual docs weighted by the inverse document frequency. This is a measure of the number of docs the words appear in weighted by inverse document frequency. We're still getting a sense of which words across the corpus and in the context of the corpus seem to have the most importance, if document frequency is a proxy for importance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx69GIqUdqtx",
        "colab_type": "text"
      },
      "source": [
        "Textacy provides access to different algorithms that can be run on docs, such as TextRank for keyword extraction. We'll start by working on a single doc, and then look at how we might scale up to thinking about the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQCRD7WTeI1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import textacy.ke"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSwQ4Rw6dWaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "key_terms_textrank = textacy.ke.textrank(corpus[4])\n",
        "key_terms_textrank"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRyHfZekewCT",
        "colab_type": "text"
      },
      "source": [
        "For comparison, we'll take a look at another algorithm, Yake. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HWvJ8ube4Vc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "key_terms_yake = textacy.ke.yake(corpus[4])\n",
        "key_terms_yake"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OarUA-BMfmV7",
        "colab_type": "text"
      },
      "source": [
        "Let's think about aggregating keywords over part of the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXNpQrZafrzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "key_terms_textrank_corpus = [textacy.ke.yake(doc) for doc in corpus[:20]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XCA9CT4f5N5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "key_terms_textrank_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4EJrV3EgBcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flat_list = [item for sublist in key_terms_textrank_corpus for item in sublist]\n",
        "flat_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3HHZLIXgawc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keyword_counter = Counter(flat_list)\n",
        "keyword_counter.most_common(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P296E9nxhnQZ",
        "colab_type": "text"
      },
      "source": [
        "### Activity:\n",
        "Let's combine a few different pieces. Try filtering the corpus on some metadata to construct a sub-corpus. Then use one of the textacy keyword algorithms to determine the most common keywords across your subcorpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_ZuOB3_h4Wn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuk7s1yZlvVi",
        "colab_type": "text"
      },
      "source": [
        "## Keyword in context\n",
        "\n",
        "One thing that researchers often find helpful in working with text is simply seeing keywords in context. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxgjXFnRla1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for doc in corpus[:20]:\n",
        "  textacy.text_utils.KWIC(doc.text, \"agriculture\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuSHX_2OVyUb",
        "colab_type": "text"
      },
      "source": [
        "## Vectorization\n",
        "\n",
        "Let's continue with corpus level analysis by taking advantage of textacy's vectorizer class, which wraps functionality from scikit-learn. We could just work directly in scikit-learn, but it can be nice for mental overhead to learn one library and be able to do a great deal with it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pFXy9VK7DXa",
        "colab_type": "text"
      },
      "source": [
        "We'll create a vectorizer, sticking with the normal term frequency defaults but discarding words that appear in less than 3 documents or more than 95% of documents. We'll also limit our features to the top 500 words according to document frequency.This means our feature set, or columns, will have a higher degree of representation across the corpus. We could vectorize according to tf-idf as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LfE48GbVyUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import textacy.vsm\n",
        "\n",
        "vectorizer = textacy.vsm.Vectorizer(min_df=3, max_df=.95, max_n_terms=500)\n",
        "tokenized_corpus = (doc._.to_terms_list(ngrams=1, as_strings=True,\n",
        "                                        filter_punct=True, \n",
        "                                        filter_stops=True, \n",
        "                                        filter_nums=True \n",
        "                                        ) for doc in corpus)\n",
        "dtm = vectorizer.fit_transform(tokenized_corpus)\n",
        "dtm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5lQ10twVyUf",
        "colab_type": "text"
      },
      "source": [
        "We have now have a matrix representation of our corpus, where rows are documents, and columns (or features) are words from the corpus. The value at any given point is the number of times that the word appears in that document. Once we have a document-term matrix, we could do a few different things with it, just within textacy, though we could take it and pass it into different algorithms within scikit-learn or other libraries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37KLE01TVyUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's first look at some of the terms\n",
        "vectorizer.terms_list[:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_I6y0JIVyUh",
        "colab_type": "text"
      },
      "source": [
        "We can see that we are still getting a number of terms which ought to be filtered out, such as numbers and punctuation. We would want to clean this up more before vectorizing in the future. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7c7uSmdnLOz",
        "colab_type": "text"
      },
      "source": [
        "## Topic Modeling\n",
        "\n",
        "Let's look quickly at one examples of what we can do with a vectorized corpus. Topic modeling is very popular for semantic exploration of texts, and there are numerous implementations. Textacy uses implementations from scikit-learn. \n",
        "\n",
        "Our corpus is rather small for topic modeling, but just to see how it's done here, we'll go ahead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF2jrntfmz8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import textacy.tm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFusfd9sm1dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = textacy.tm.TopicModel(\"lda\", n_topics=10)\n",
        "model.fit(dtm)\n",
        "doc_topic_matrix = model.transform(dtm)\n",
        "doc_topic_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zP_oC22nJLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, top_n=10):\n",
        "  print(\"topic\", topic_idx, \":\", \"   \".join(top_terms))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmNCMQIJnoKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}